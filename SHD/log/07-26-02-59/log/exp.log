===== Exp configuration =====

neuron_type:		RadLIF
nb_inputs:		700
nb_outputs:		20
nb_layers:		3
nb_hiddens:		128
nb_steps:		100
pdrop:		0.1
normalization:		batchnorm
use_bias:		False
bidirectional:		False
date:		07-26-02-59
use_pretrained_model:		False
only_do_testing:		False
load_exp_folder:		None
new_exp_folder:		./log/07-26-02-59/
dataset_name:		shd
data_folder:		data/raw/
save_best:		True
batch_size:		128
nb_epochs:		50
start_epoch:		0
lr:		0.01
scheduler_patience:		1
scheduler_factor:		0.7
use_regularizers:		False
reg_factor:		0.5
reg_fmin:		0.01
reg_fmax:		0.5
use_augm:		False
use_readout_layer:		True
threshold:		1.0
device:		cuda

Device is set to cuda

Number of examples in train set: 8156
SHD does not have a validation split. Using test split.
Number of examples in test set: 2264

Created new spiking model:
 SNN(
  (snn): ModuleList(
    (0): RadLIFLayer(
      (W): Linear(in_features=700, out_features=128, bias=False)
      (V): Linear(in_features=128, out_features=128, bias=False)
      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): RadLIFLayer(
      (W): Linear(in_features=128, out_features=128, bias=False)
      (V): Linear(in_features=128, out_features=128, bias=False)
      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): ReadoutLayer(
      (W): Linear(in_features=128, out_features=20, bias=False)
      (norm): BatchNorm1d(20, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
)

Total number of trainable parameters is 142908

------ Begin training ------

Epoch 1: train loss=3.1092579402029514, acc=0.17534073539402173, mean act rate=0.059095896780490875, lr=0.01, time=0:00:33.557677
Epoch 1: valid loss=2.264819065729777, acc=0.283499053030303, mean act rate=0.0680578202009201

Best model saved with valid acc=0.283499053030303

-----------------------------

Epoch 2: train loss=1.4011738579720259, acc=0.5353791610054348, mean act rate=0.06729794293642044, lr=0.01, time=0:00:30.932691
Epoch 2: valid loss=1.4210821919971042, acc=0.5565025252525253, mean act rate=0.07895233482122421

Best model saved with valid acc=0.5565025252525253

-----------------------------

Epoch 3: train loss=0.8118024999275804, acc=0.7320238196331521, mean act rate=0.07327602803707123, lr=0.01, time=0:00:30.639398
Epoch 3: valid loss=0.6601901236507628, acc=0.7882733585858586, mean act rate=0.07885963469743729

Best model saved with valid acc=0.7882733585858586

-----------------------------

Epoch 4: train loss=0.5112872370518744, acc=0.825943656589674, mean act rate=0.08227766305208206, lr=0.01, time=0:00:31.181350
Epoch 4: valid loss=0.7193543480502235, acc=0.7875236742424243, mean act rate=0.0875033289194107

-----------------------------

Epoch 5: train loss=0.3904588685836643, acc=0.8710565981657609, mean act rate=0.08902107924222946, lr=0.01, time=0:00:34.848464
Epoch 5: valid loss=0.5962802155150307, acc=0.8066603535353535, mean act rate=0.10506746172904968

Best model saved with valid acc=0.8066603535353535

-----------------------------

Epoch 6: train loss=0.314687657635659, acc=0.8954971976902174, mean act rate=0.1056232899427414, lr=0.01, time=0:00:49.826517
Epoch 6: valid loss=0.9884992672337426, acc=0.7097143308080809, mean act rate=0.12333310395479202

-----------------------------

Epoch 7: train loss=0.28740814654156566, acc=0.9052309782608696, mean act rate=0.11658374965190887, lr=0.01, time=0:00:31.789089
Epoch 7: valid loss=0.374440214700169, acc=0.8797743055555556, mean act rate=0.11873120814561844

Best model saved with valid acc=0.8797743055555556

-----------------------------

Epoch 8: train loss=0.22796729195397347, acc=0.9256644870923914, mean act rate=0.11297011375427246, lr=0.01, time=0:00:28.594592
Epoch 8: valid loss=0.5958729866478178, acc=0.823666351010101, mean act rate=0.12206254154443741

-----------------------------

Epoch 9: train loss=0.23871315456926823, acc=0.9201341711956521, mean act rate=0.12240001559257507, lr=0.01, time=0:00:33.711537
Epoch 9: valid loss=0.5576149109337065, acc=0.8290719696969697, mean act rate=0.12286367267370224

-----------------------------

Epoch 10: train loss=0.16352023812942207, acc=0.9470957880434783, mean act rate=0.11868825554847717, lr=0.006999999999999999, time=0:00:29.115777
Epoch 10: valid loss=0.4243539687660005, acc=0.8658854166666666, mean act rate=0.1206233948469162

-----------------------------

Epoch 11: train loss=0.13774678716436028, acc=0.9568401834239131, mean act rate=0.12084315717220306, lr=0.006999999999999999, time=0:00:29.699365
Epoch 11: valid loss=0.39089807619651157, acc=0.8806818181818181, mean act rate=0.12613545358181

Best model saved with valid acc=0.8806818181818181

-----------------------------

Epoch 12: train loss=0.11648560524918139, acc=0.9615266219429348, mean act rate=0.11960145086050034, lr=0.006999999999999999, time=0:00:46.831394
Epoch 12: valid loss=0.5469223509232203, acc=0.8539299242424243, mean act rate=0.11923742294311523

-----------------------------

Epoch 13: train loss=0.09837403224082664, acc=0.9676566745923914, mean act rate=0.11836683750152588, lr=0.006999999999999999, time=0:00:53.550690
Epoch 13: valid loss=0.3741861863268746, acc=0.890546085858586, mean act rate=0.1219358891248703

Best model saved with valid acc=0.890546085858586

-----------------------------

Epoch 14: train loss=0.07831308158347383, acc=0.9738079568614131, mean act rate=0.11799904704093933, lr=0.006999999999999999, time=0:00:44.153652
Epoch 14: valid loss=0.4043884070383178, acc=0.8760653409090909, mean act rate=0.12178049236536026

-----------------------------

Epoch 15: train loss=0.07826970968744718, acc=0.9734152088994565, mean act rate=0.11888334155082703, lr=0.006999999999999999, time=0:00:35.143307
Epoch 15: valid loss=0.529469789730178, acc=0.8725536616161615, mean act rate=0.12325409054756165

-----------------------------

Epoch 16: train loss=0.08471182899666019, acc=0.9730755349864131, mean act rate=0.12296955287456512, lr=0.004899999999999999, time=0:00:35.356324
Epoch 16: valid loss=0.31798339469565284, acc=0.9029356060606061, mean act rate=0.1340692788362503

Best model saved with valid acc=0.9029356060606061

-----------------------------

Epoch 17: train loss=0.07491060133907013, acc=0.9770030146059783, mean act rate=0.12901178002357483, lr=0.004899999999999999, time=0:00:33.013598
Epoch 17: valid loss=0.4191685939828555, acc=0.8897569444444444, mean act rate=0.1315118670463562

-----------------------------

Epoch 18: train loss=0.06487953418400139, acc=0.9799592391304348, mean act rate=0.12749820947647095, lr=0.004899999999999999, time=0:00:32.180645
Epoch 18: valid loss=0.373868852853775, acc=0.8918876262626263, mean act rate=0.12944090366363525

-----------------------------

Epoch 19: train loss=0.042358061895356514, acc=0.9878672724184783, mean act rate=0.1266757696866989, lr=0.003429999999999999, time=0:00:31.694759
Epoch 19: valid loss=0.36781637205017936, acc=0.9005287247474748, mean act rate=0.13178174197673798

-----------------------------

Epoch 20: train loss=0.038350391638232395, acc=0.9879150390625, mean act rate=0.12791121006011963, lr=0.003429999999999999, time=0:00:32.095542
Epoch 20: valid loss=0.4146748897102144, acc=0.8959911616161617, mean act rate=0.13296149671077728

-----------------------------

Epoch 21: train loss=0.033402166998712346, acc=0.9903564453125, mean act rate=0.12820348143577576, lr=0.002400999999999999, time=0:00:32.135667
Epoch 21: valid loss=0.48614251448048484, acc=0.8706597222222222, mean act rate=0.13136237859725952

-----------------------------

Epoch 22: train loss=0.029961004816868808, acc=0.9912109375, mean act rate=0.1292552500963211, lr=0.002400999999999999, time=0:00:32.486903
Epoch 22: valid loss=0.40419602890809375, acc=0.8925583964646465, mean act rate=0.13267290592193604

-----------------------------

Epoch 23: train loss=0.030889881862094626, acc=0.9901866083559783, mean act rate=0.12729962170124054, lr=0.0016806999999999992, time=0:00:31.459896
Epoch 23: valid loss=0.444044877257612, acc=0.8856928661616162, mean act rate=0.13038520514965057

-----------------------------

Epoch 24: train loss=0.025985851818404626, acc=0.9921875, mean act rate=0.12810561060905457, lr=0.0016806999999999992, time=0:00:36.474606
Epoch 24: valid loss=0.43321528865231407, acc=0.8908222853535354, mean act rate=0.13215012848377228

-----------------------------

Epoch 25: train loss=0.024847714805218857, acc=0.9928243885869565, mean act rate=0.1286308467388153, lr=0.0011764899999999994, time=0:00:45.796972
Epoch 25: valid loss=0.4344329718086455, acc=0.8880208333333334, mean act rate=0.13208158314228058

-----------------------------

Epoch 26: train loss=0.02405669469590066, acc=0.9938487177309783, mean act rate=0.1284629851579666, lr=0.0011764899999999994, time=0:00:43.880421
Epoch 26: valid loss=0.40364953461620545, acc=0.8970959595959597, mean act rate=0.13173121213912964

-----------------------------

Epoch 27: train loss=0.02290771753905574, acc=0.9934347401494565, mean act rate=0.1280686855316162, lr=0.0008235429999999996, time=0:00:41.678531
Epoch 27: valid loss=0.41443638420767254, acc=0.897293244949495, mean act rate=0.13130797445774078

-----------------------------

Epoch 28: train loss=0.025647886068327352, acc=0.992431640625, mean act rate=0.1280507594347, lr=0.0008235429999999996, time=0:00:43.336771
Epoch 28: valid loss=0.43328987807035446, acc=0.8860874368686869, mean act rate=0.1322822868824005

-----------------------------

Epoch 29: train loss=0.02249676266364986, acc=0.9936311141304348, mean act rate=0.12854595482349396, lr=0.0005764800999999997, time=0:00:35.077299
Epoch 29: valid loss=0.41246799296802944, acc=0.8910590277777778, mean act rate=0.13190144300460815

-----------------------------

Epoch 30: train loss=0.019381559748580912, acc=0.9952392578125, mean act rate=0.12861143052577972, lr=0.0005764800999999997, time=0:00:38.634586
Epoch 30: valid loss=0.3979026919437779, acc=0.8986742424242424, mean act rate=0.13234028220176697

-----------------------------

