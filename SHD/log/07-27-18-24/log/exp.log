===== Exp configuration =====
neuron_type:		RadLIF
nb_inputs:		700
nb_outputs:		20
nb_layers:		3
nb_hiddens:		1024
nb_steps:		100
pdrop:		0.1
normalization:		batchnorm
use_bias:		False
bidirectional:		False
date:		07-27-18-24
use_pretrained_model:		False
only_do_testing:		False
load_exp_folder:		None
new_exp_folder:		./log/07-27-18-24/
dataset_name:		shd
data_folder:		data/raw/
save_best:		True
batch_size:		128
nb_epochs:		50
dropout:		0.75
dropout_stop:		0.95
dropout_stepping:		0.02
ckpt_freq:		5
start_epoch:		0
lr:		0.01
scheduler_patience:		2
scheduler_factor:		0.7
use_regularizers:		False
reg_factor:		0.5
reg_fmin:		0.01
reg_fmax:		0.1
use_augm:		False
use_readout_layer:		True
threshold:		1.0
device:		cuda

Device is set to cuda

Number of examples in train set: 8156
SHD does not have a validation split. Using test split.
Number of examples in test set: 2264

Created new spiking model:
 SNN(
  (snn): ModuleList(
    (0): RadLIFLayer(
      (W): Linear(in_features=700, out_features=1024, bias=False)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): RadLIFLayer(
      (W): Linear(in_features=1024, out_features=1024, bias=False)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): ReadoutLayer(
      (W): Linear(in_features=1024, out_features=20, bias=False)
      (norm): BatchNorm1d(20, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
)

Total number of trainable parameters is 3895356

------ Begin training ------

Epoch 1: train loss=1.8388569634407759, acc=0.457084324048913, fr=0.07285969704389572, lr=0.01, time=0:00:36.739961, cin=0.002868195530027151, cout=0.01452753134071827
Epoch 1: valid loss=1.4732432497872248, acc=0.5664457070707071, fr=0.08069887012243271, mask=0.875, time=0:00:04.412170

Best model saved with valid acc=0.5664457070707071

-----------------------------

Epoch 2: train loss=0.4976994823664427, acc=0.8343399711277174, fr=0.07885310053825378, lr=0.01, time=0:00:30.142320, cin=0.0023626124020665884, cout=0.026549886912107468
Epoch 2: valid loss=0.6902000837855868, acc=0.7871685606060607, fr=0.08171199262142181, mask=0.8784151077270508, time=0:00:04.203441

Best model saved with valid acc=0.7871685606060607

-----------------------------

Epoch 3: train loss=0.24946737883146852, acc=0.9237113620923914, fr=0.07856126874685287, lr=0.01, time=0:00:29.031446, cin=0.003306460101157427, cout=0.016333067789673805
Epoch 3: valid loss=0.36448388132784104, acc=0.878353851010101, fr=0.08235247433185577, mask=0.8807663917541504, time=0:00:04.212784

Best model saved with valid acc=0.878353851010101

-----------------------------

Epoch 4: train loss=0.16686276905238628, acc=0.9465650475543479, fr=0.07790626585483551, lr=0.01, time=0:00:31.983703, cin=0.0020288887899369, cout=0.006167818792164326
Epoch 4: valid loss=0.4626988470554352, acc=0.8583491161616162, fr=0.08207609504461288, mask=0.883150577545166, time=0:00:04.223898

-----------------------------

Epoch 5: train loss=0.14205876202322543, acc=0.9538361922554348, fr=0.0787268579006195, lr=0.01, time=0:00:29.424770, cin=0.004251002799719572, cout=0.007642097305506468
Epoch 5: valid loss=0.4761033207178116, acc=0.8521938131313131, fr=0.08365241438150406, mask=0.8854656219482422, time=0:00:04.218754
