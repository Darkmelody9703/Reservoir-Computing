
        Training Config
        ---------------
        Date: 08-18-17-16
        Use pretrained model: False
        Only do testing: False
        Load experiment folder: None
        New experiment folder: ./log/08-18-17-16
        Dataset name: shd
        Data folder: ./data/raw/
        Log to file: True
        Save best model: True
        Batch size: 512
        Number of epochs: 50
        Start epoch: 0
        Initial learning rate: 0.01
        Scheduler patience: 1
        Scheduler factor: 0.7
        Use regularizers: True
        Regularization factor: 0.5
        Regularization min firing rate: 0.01
        Reguarization max firing rate: 0.2
        Use data augmentation: False
        Number of steps: 50
        Trials: 5
        Seed: 1692350162
        Checkpoint frequency: 5
        Threshold: 1.0
        
        ---------------
        Model Config
        
        Model Type: RadLIF
        Number of layers: 3
        Number of hidden neurons: 1024
        Dropout rate: 0.1
        Normalization: batchnorm
        Use bias: True
        Bidirectional: False
        Train input layer: True
        Dropout: 0.0
        Dropout_stop: 0.95
        Dropout_stepping: 0.0
        Clustering: False
        Clustering factor: [1, 2.5]
        Cin min and max: [0.01, 0.05]
        Cout min and max: [0.05, 0.2]
        Number of clusters: 8
        Noise in testset: 0.0
    

Device is set to cuda

Number of examples in train set: 8156
SHD does not have a validation split. Using test split.
Number of examples in test set: 2264

Created new spiking model:
 SNN(
  (snn): ModuleList(
    (0): RadLIFLayer(
      (W): Linear(in_features=700, out_features=1024, bias=True)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): RadLIFLayer(
      (W): Linear(in_features=1024, out_features=1024, bias=True)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): ReadoutLayer(
      (W): Linear(in_features=1024, out_features=20, bias=True)
      (norm): BatchNorm1d(20, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
)

Total number of trainable parameters is 3897424

---------------Trial:1---------------


------ Begin training ------

Epoch 1: train loss=16.4755, acc=0.0514, fr=0.0460, lr=0.0100, time=16.629281
Epoch 1: valid loss=3.0291, acc=0.0499, fr=0.0519, mask=0.8750, time=03.548825

Best model saved with valid acc=0.04992766203703704

-----------------------------

Epoch 2: train loss=16.4940, acc=0.0489, fr=0.0460, lr=0.0100, time=13.222913
Epoch 2: valid loss=3.0570, acc=0.0522, fr=0.0521, mask=0.8750, time=03.462197

Best model saved with valid acc=0.052170138888888884

-----------------------------

Epoch 3: train loss=16.5290, acc=0.0459, fr=0.0460, lr=0.0100, time=12.399394
Epoch 3: valid loss=3.1002, acc=0.0620, fr=0.0525, mask=0.8750, time=03.335617

Best model saved with valid acc=0.06197916666666666

-----------------------------

Epoch 4: train loss=16.5010, acc=0.0514, fr=0.0460, lr=0.0100, time=13.347103
Epoch 4: valid loss=3.1946, acc=0.0537, fr=0.0528, mask=0.8750, time=03.878264

-----------------------------

Epoch 5: train loss=16.4694, acc=0.0481, fr=0.0460, lr=0.0100, time=14.209033
Epoch 5: valid loss=3.3311, acc=0.0542, fr=0.0530, mask=0.8750, time=03.282260

-----------------------------

Epoch 6: train loss=16.4583, acc=0.0537, fr=0.0460, lr=0.0070, time=12.495655
Epoch 6: valid loss=3.6166, acc=0.0456, fr=0.0535, mask=0.8750, time=03.236284

-----------------------------

Epoch 7: train loss=16.4663, acc=0.0520, fr=0.0460, lr=0.0070, time=13.547247
Epoch 7: valid loss=3.8344, acc=0.0433, fr=0.0538, mask=0.8750, time=03.401961

-----------------------------

Epoch 8: train loss=16.4613, acc=0.0466, fr=0.0459, lr=0.0049, time=13.407760
Epoch 8: valid loss=3.9624, acc=0.0524, fr=0.0540, mask=0.8750, time=03.191704

-----------------------------

Epoch 9: train loss=16.4006, acc=0.0522, fr=0.0459, lr=0.0049, time=11.742744
Epoch 9: valid loss=4.1170, acc=0.0435, fr=0.0540, mask=0.8750, time=03.147503

-----------------------------

Epoch 10: train loss=16.4841, acc=0.0513, fr=0.0460, lr=0.0034, time=11.885637
Epoch 10: valid loss=4.0982, acc=0.0508, fr=0.0539, mask=0.8750, time=03.234845

-----------------------------

Epoch 11: train loss=16.4897, acc=0.0518, fr=0.0460, lr=0.0034, time=11.773803
Epoch 11: valid loss=4.1016, acc=0.0571, fr=0.0540, mask=0.8750, time=03.134817

-----------------------------

Epoch 12: train loss=16.5651, acc=0.0479, fr=0.0460, lr=0.0024, time=11.885495
Epoch 12: valid loss=4.0971, acc=0.0451, fr=0.0540, mask=0.8750, time=03.314194

-----------------------------

Epoch 13: train loss=16.4451, acc=0.0503, fr=0.0460, lr=0.0024, time=11.944387
Epoch 13: valid loss=4.1946, acc=0.0479, fr=0.0540, mask=0.8750, time=03.230114

-----------------------------

Epoch 14: train loss=16.5053, acc=0.0481, fr=0.0460, lr=0.0017, time=12.765644
Epoch 14: valid loss=4.0967, acc=0.0512, fr=0.0540, mask=0.8750, time=04.061459

-----------------------------

