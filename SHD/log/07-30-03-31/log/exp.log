===== Exp configuration =====
neuron_type:		RadLIF
nb_inputs:		700
nb_outputs:		20
nb_layers:		3
nb_hiddens:		1024
nb_steps:		100
pdrop:		0.1
normalization:		batchnorm
use_bias:		False
bidirectional:		False
date:		07-30-03-31
use_pretrained_model:		False
only_do_testing:		False
load_exp_folder:		None
new_exp_folder:		./log/07-30-03-31/
dataset_name:		shd
data_folder:		data/raw/
batch_size:		128
nb_epochs:		50
dropout:		0.75
dropout_stop:		0.95
dropout_stepping:		0.02
ckpt_freq:		5
clustering:		True
clustering_factor:		[0.1, 0.25]
cin_minmax:		[0.001, 0.05]
cout_minmax:		[0.05, 0.2]
noise_test:		0.2
start_epoch:		0
lr:		0.01
scheduler_patience:		2
scheduler_factor:		0.7
use_regularizers:		False
reg_factor:		0.5
reg_fmin:		0.01
reg_fmax:		0.1
use_augm:		False
use_readout_layer:		True
threshold:		1.0
device:		cuda

Device is set to cuda
checkpoint_dir: ./log/07-30-03-31//checkpoints/

Number of examples in train set: 8156
SHD does not have a validation split. Using test split.
Number of examples in test set: 2264

Created new spiking model:
 SNN(
  (snn): ModuleList(
    (0): RadLIFLayer(
      (W): Linear(in_features=700, out_features=1024, bias=False)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): RadLIFLayer(
      (W): Linear(in_features=1024, out_features=1024, bias=False)
      (V): Linear(in_features=1024, out_features=1024, bias=False)
      (norm): BatchNorm1d(1024, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): ReadoutLayer(
      (W): Linear(in_features=1024, out_features=20, bias=False)
      (norm): BatchNorm1d(20, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
)

Total number of trainable parameters is 3895356

------ Begin training ------

Epoch 1: train loss=2.0285, acc=0.3974, fr=0.0722, lr=0.0100, time=0:00:34.333851, cin=0.009930, 0.006138, cout=0.091964, 0.066674
Epoch 1: valid loss=1.8194, acc=0.5047, fr=0.0787, mask=0.8750, time=0:00:05.527353

-----------------------------

Epoch 2: train loss=0.6151, acc=0.7946, fr=0.0739, lr=0.0100, time=0:00:30.276249, cin=0.008488, 0.004505, cout=0.166128, 0.127632
Epoch 2: valid loss=0.8801, acc=0.7274, fr=0.0793, mask=0.8785, time=0:00:05.616614

-----------------------------

Epoch 3: train loss=0.3072, acc=0.9063, fr=0.0749, lr=0.0100, time=0:00:30.325910, cin=0.004558, 0.006275, cout=0.124624, 0.115803
Epoch 3: valid loss=0.4957, acc=0.8243, fr=0.0762, mask=0.8809, time=0:00:05.460161

-----------------------------

Epoch 4: train loss=0.2394, acc=0.9263, fr=0.0717, lr=0.0100, time=0:00:31.730337, cin=0.005983, 0.005063, cout=0.106496, 0.108427
Epoch 4: valid loss=0.6725, acc=0.8001, fr=0.0765, mask=0.8833, time=0:00:06.155898

-----------------------------

Epoch 5: train loss=0.1412, acc=0.9564, fr=0.0742, lr=0.0100, time=0:00:36.450370, cin=0.009112, 0.004199, cout=0.112367, 0.101497
Epoch 5: valid loss=0.3845, acc=0.8762, fr=0.0792, mask=0.8856, time=0:00:05.958787

Best model saved with valid acc=0.8761837121212122

-----------------------------

Epoch 6: train loss=0.1044, acc=0.9669, fr=0.0752, lr=0.0100, time=0:00:31.950011, cin=0.016215, 0.004604, cout=0.106822, 0.097294
Epoch 6: valid loss=0.3357, acc=0.8945, fr=0.0788, mask=0.8879, time=0:00:05.999815

Best model saved with valid acc=0.89453125

-----------------------------

Epoch 7: train loss=0.0937, acc=0.9709, fr=0.0735, lr=0.0100, time=0:00:32.015960, cin=0.013645, 0.003732, cout=0.134419, 0.090123
Epoch 7: valid loss=0.5191, acc=0.8648, fr=0.0766, mask=0.8902, time=0:00:05.968232

-----------------------------

Epoch 8: train loss=0.0620, acc=0.9818, fr=0.0700, lr=0.0100, time=0:00:31.395550, cin=0.013006, 0.002878, cout=0.137442, 0.086325
Epoch 8: valid loss=0.4580, acc=0.8759, fr=0.0737, mask=0.8924, time=0:00:05.917270

-----------------------------

Epoch 9: train loss=0.0630, acc=0.9803, fr=0.0683, lr=0.0100, time=0:00:31.498663, cin=0.010666, 0.002657, cout=0.152282, 0.083421
Epoch 9: valid loss=0.4803, acc=0.8800, fr=0.0712, mask=0.8946, time=0:00:05.849120

-----------------------------

Epoch 10: train loss=0.0331, acc=0.9918, fr=0.0679, lr=0.0070, time=0:00:31.479329, cin=0.010719, 0.002749, cout=0.140719, 0.082859
Epoch 10: valid loss=0.3663, acc=0.8891, fr=0.0719, mask=0.8967, time=0:00:05.971334

-----------------------------

Epoch 11: train loss=0.0308, acc=0.9904, fr=0.0676, lr=0.0070, time=0:00:31.385967, cin=0.009567, 0.002468, cout=0.147796, 0.080717
Epoch 11: valid loss=0.5092, acc=0.8605, fr=0.0718, mask=0.8988, time=0:00:06.521722

-----------------------------

Epoch 12: train loss=0.0231, acc=0.9927, fr=0.0680, lr=0.0070, time=0:00:31.780893, cin=0.007892, 0.002295, cout=0.159094, 0.081714
Epoch 12: valid loss=0.5106, acc=0.8778, fr=0.0719, mask=0.9008, time=0:00:05.884525

-----------------------------

Epoch 13: train loss=0.0146, acc=0.9966, fr=0.0678, lr=0.0049, time=0:00:32.772529, cin=0.008238, 0.002655, cout=0.158984, 0.083240
Epoch 13: valid loss=0.4887, acc=0.8900, fr=0.0718, mask=0.9028, time=0:00:06.426936

-----------------------------

Epoch 14: train loss=0.0125, acc=0.9961, fr=0.0676, lr=0.0049, time=0:00:33.429610, cin=0.008336, 0.002795, cout=0.156138, 0.081044
Epoch 14: valid loss=0.4322, acc=0.8926, fr=0.0714, mask=0.9047, time=0:00:06.118990

-----------------------------

