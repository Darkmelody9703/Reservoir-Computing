{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.nn import EGATConv, EdgeWeightNorm, GraphConv\n",
    "from dgl.utils import expand_as_pair\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=64, \n",
    "                                           nhead=4,\n",
    "                                           dim_feedforward=128,)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "src = torch.rand(10, 32, 64) # batch, channel, d_model\n",
    "out = transformer_encoder(src)\n",
    "\n",
    "avgpool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "out = avgpool(out)\n",
    "out = torch.flatten(out, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   learn_dgl.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=64, \n",
    "                                           nhead=8,\n",
    "                                           dim_feedforward=128,)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=1)\n",
    "memory = torch.rand(10, 32, 64)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in avgpool.named_parameters():\n",
    "    print(i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generation Finish\n",
      "finish inference\n",
      "finish inference\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import inference_new\n",
    "run_time = time.strftime(\"%Y.%m.%d-%H-%M-%S\", time.localtime())\n",
    "# param_search(run_time)\n",
    "from config import Config\n",
    "from data import part_DATA\n",
    "from RC import torchRC, EGAT, EGCN\n",
    "import dgl\n",
    "from RC import MLP\n",
    "\n",
    "config = Config()\n",
    "config.data = 'mnist'\n",
    "config.train_num = 10000\n",
    "config.test_num = 2000\n",
    "config.N_in = 28*28\n",
    "config.N_out = 10\n",
    "train_loader, test_loader = part_DATA(config)\n",
    "\n",
    "model = torchRC(config).to(config.device)\n",
    "train_rs, train_label = inference_new(model, config, train_loader,)\n",
    "test_rs, test_label = inference_new(model, config, test_loader,)\n",
    "train_rs = train_rs[:,1:,:].mean(1)\n",
    "test_rs = test_rs[:,1:,:].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,300] loss:1.843, train acc:0.4879, test acc:0.6850\n",
      "[2,300] loss:1.175, train acc:0.7510, test acc:0.7850\n",
      "[3,300] loss:0.856, train acc:0.8098, test acc:0.8200\n",
      "[4,300] loss:0.688, train acc:0.8372, test acc:0.8465\n",
      "[5,300] loss:0.593, train acc:0.8537, test acc:0.8595\n",
      "[6,300] loss:0.532, train acc:0.8639, test acc:0.8715\n",
      "[7,300] loss:0.489, train acc:0.8716, test acc:0.8770\n",
      "[8,300] loss:0.456, train acc:0.8772, test acc:0.8835\n",
      "[9,300] loss:0.430, train acc:0.8825, test acc:0.8865\n",
      "[10,300] loss:0.408, train acc:0.8875, test acc:0.8920\n",
      "[11,300] loss:0.390, train acc:0.8912, test acc:0.8930\n",
      "[12,300] loss:0.375, train acc:0.8948, test acc:0.8945\n",
      "[13,300] loss:0.360, train acc:0.8995, test acc:0.8970\n",
      "[14,300] loss:0.348, train acc:0.9036, test acc:0.8990\n",
      "[15,300] loss:0.336, train acc:0.9064, test acc:0.9010\n",
      "[16,300] loss:0.327, train acc:0.9096, test acc:0.9030\n",
      "[17,300] loss:0.318, train acc:0.9125, test acc:0.9065\n",
      "[18,300] loss:0.309, train acc:0.9152, test acc:0.9070\n",
      "[19,300] loss:0.302, train acc:0.9169, test acc:0.9075\n",
      "[20,300] loss:0.295, train acc:0.9189, test acc:0.9080\n",
      "[21,300] loss:0.288, train acc:0.9215, test acc:0.9085\n",
      "[22,300] loss:0.282, train acc:0.9229, test acc:0.9095\n",
      "[23,300] loss:0.276, train acc:0.9248, test acc:0.9095\n",
      "[24,300] loss:0.270, train acc:0.9264, test acc:0.9100\n",
      "[25,300] loss:0.265, train acc:0.9280, test acc:0.9105\n",
      "[26,300] loss:0.260, train acc:0.9294, test acc:0.9105\n",
      "[27,300] loss:0.255, train acc:0.9308, test acc:0.9120\n",
      "[28,300] loss:0.250, train acc:0.9320, test acc:0.9120\n",
      "[29,300] loss:0.246, train acc:0.9328, test acc:0.9130\n",
      "[30,300] loss:0.242, train acc:0.9335, test acc:0.9145\n",
      "[31,300] loss:0.238, train acc:0.9342, test acc:0.9160\n",
      "[32,300] loss:0.234, train acc:0.9356, test acc:0.9165\n",
      "[33,300] loss:0.230, train acc:0.9369, test acc:0.9180\n",
      "[34,300] loss:0.226, train acc:0.9377, test acc:0.9195\n",
      "[35,300] loss:0.223, train acc:0.9383, test acc:0.9195\n",
      "[36,300] loss:0.219, train acc:0.9390, test acc:0.9195\n",
      "[37,300] loss:0.216, train acc:0.9402, test acc:0.9215\n",
      "[38,300] loss:0.213, train acc:0.9415, test acc:0.9230\n",
      "[39,300] loss:0.209, train acc:0.9420, test acc:0.9230\n",
      "[40,300] loss:0.206, train acc:0.9424, test acc:0.9235\n",
      "[41,300] loss:0.203, train acc:0.9431, test acc:0.9235\n",
      "[42,300] loss:0.200, train acc:0.9440, test acc:0.9230\n",
      "[43,300] loss:0.197, train acc:0.9454, test acc:0.9230\n",
      "[44,300] loss:0.195, train acc:0.9454, test acc:0.9245\n",
      "[45,300] loss:0.192, train acc:0.9465, test acc:0.9250\n",
      "[46,300] loss:0.189, train acc:0.9470, test acc:0.9250\n",
      "[47,300] loss:0.187, train acc:0.9476, test acc:0.9250\n",
      "[48,300] loss:0.184, train acc:0.9485, test acc:0.9260\n",
      "[49,300] loss:0.182, train acc:0.9497, test acc:0.9265\n",
      "[50,300] loss:0.179, train acc:0.9506, test acc:0.9270\n",
      "[51,300] loss:0.177, train acc:0.9514, test acc:0.9275\n",
      "[52,300] loss:0.174, train acc:0.9522, test acc:0.9270\n",
      "[53,300] loss:0.172, train acc:0.9530, test acc:0.9270\n",
      "[54,300] loss:0.170, train acc:0.9535, test acc:0.9265\n",
      "[55,300] loss:0.168, train acc:0.9539, test acc:0.9265\n",
      "[56,300] loss:0.165, train acc:0.9543, test acc:0.9275\n",
      "[57,300] loss:0.163, train acc:0.9548, test acc:0.9275\n",
      "[58,300] loss:0.161, train acc:0.9556, test acc:0.9275\n",
      "[59,300] loss:0.159, train acc:0.9561, test acc:0.9275\n",
      "[60,300] loss:0.157, train acc:0.9566, test acc:0.9275\n",
      "[61,300] loss:0.155, train acc:0.9570, test acc:0.9280\n",
      "[62,300] loss:0.153, train acc:0.9581, test acc:0.9280\n",
      "[63,300] loss:0.151, train acc:0.9590, test acc:0.9290\n",
      "[64,300] loss:0.149, train acc:0.9593, test acc:0.9290\n",
      "[65,300] loss:0.147, train acc:0.9602, test acc:0.9290\n",
      "[66,300] loss:0.146, train acc:0.9610, test acc:0.9290\n",
      "[67,300] loss:0.144, train acc:0.9612, test acc:0.9295\n",
      "[68,300] loss:0.142, train acc:0.9621, test acc:0.9295\n",
      "[69,300] loss:0.140, train acc:0.9628, test acc:0.9300\n",
      "[70,300] loss:0.139, train acc:0.9635, test acc:0.9305\n",
      "[71,300] loss:0.137, train acc:0.9638, test acc:0.9315\n",
      "[72,300] loss:0.135, train acc:0.9642, test acc:0.9320\n",
      "[73,300] loss:0.134, train acc:0.9645, test acc:0.9325\n",
      "[74,300] loss:0.132, train acc:0.9651, test acc:0.9330\n",
      "[75,300] loss:0.130, train acc:0.9657, test acc:0.9335\n",
      "[76,300] loss:0.129, train acc:0.9666, test acc:0.9345\n",
      "[77,300] loss:0.127, train acc:0.9674, test acc:0.9340\n",
      "[78,300] loss:0.126, train acc:0.9676, test acc:0.9340\n",
      "[79,300] loss:0.124, train acc:0.9683, test acc:0.9340\n",
      "[80,300] loss:0.123, train acc:0.9696, test acc:0.9345\n",
      "[81,300] loss:0.121, train acc:0.9699, test acc:0.9345\n",
      "[82,300] loss:0.120, train acc:0.9705, test acc:0.9355\n",
      "[83,300] loss:0.118, train acc:0.9716, test acc:0.9350\n",
      "[84,300] loss:0.117, train acc:0.9723, test acc:0.9360\n",
      "[85,300] loss:0.115, train acc:0.9732, test acc:0.9365\n",
      "[86,300] loss:0.114, train acc:0.9736, test acc:0.9365\n",
      "[87,300] loss:0.113, train acc:0.9739, test acc:0.9370\n",
      "[88,300] loss:0.111, train acc:0.9742, test acc:0.9365\n",
      "[89,300] loss:0.110, train acc:0.9752, test acc:0.9370\n",
      "[90,300] loss:0.109, train acc:0.9760, test acc:0.9370\n",
      "[91,300] loss:0.107, train acc:0.9765, test acc:0.9365\n",
      "[92,300] loss:0.106, train acc:0.9769, test acc:0.9365\n",
      "[93,300] loss:0.105, train acc:0.9771, test acc:0.9360\n",
      "[94,300] loss:0.104, train acc:0.9773, test acc:0.9360\n",
      "[95,300] loss:0.102, train acc:0.9777, test acc:0.9360\n",
      "[96,300] loss:0.101, train acc:0.9781, test acc:0.9365\n",
      "[97,300] loss:0.100, train acc:0.9783, test acc:0.9355\n",
      "[98,300] loss:0.099, train acc:0.9785, test acc:0.9360\n",
      "[99,300] loss:0.098, train acc:0.9789, test acc:0.9360\n",
      "[100,300] loss:0.096, train acc:0.9800, test acc:0.9360\n",
      "[101,300] loss:0.095, train acc:0.9803, test acc:0.9360\n",
      "[102,300] loss:0.094, train acc:0.9807, test acc:0.9360\n",
      "[103,300] loss:0.093, train acc:0.9811, test acc:0.9360\n",
      "[104,300] loss:0.092, train acc:0.9816, test acc:0.9360\n",
      "[105,300] loss:0.091, train acc:0.9818, test acc:0.9360\n",
      "[106,300] loss:0.090, train acc:0.9822, test acc:0.9355\n",
      "[107,300] loss:0.088, train acc:0.9825, test acc:0.9355\n",
      "[108,300] loss:0.087, train acc:0.9827, test acc:0.9360\n",
      "[109,300] loss:0.086, train acc:0.9834, test acc:0.9360\n",
      "[110,300] loss:0.085, train acc:0.9838, test acc:0.9365\n",
      "[111,300] loss:0.084, train acc:0.9846, test acc:0.9365\n",
      "[112,300] loss:0.083, train acc:0.9852, test acc:0.9365\n",
      "[113,300] loss:0.082, train acc:0.9853, test acc:0.9365\n",
      "[114,300] loss:0.081, train acc:0.9854, test acc:0.9360\n",
      "[115,300] loss:0.080, train acc:0.9855, test acc:0.9355\n",
      "[116,300] loss:0.079, train acc:0.9859, test acc:0.9355\n",
      "[117,300] loss:0.078, train acc:0.9861, test acc:0.9355\n",
      "[118,300] loss:0.077, train acc:0.9863, test acc:0.9355\n",
      "[119,300] loss:0.076, train acc:0.9866, test acc:0.9355\n",
      "[120,300] loss:0.075, train acc:0.9866, test acc:0.9350\n",
      "[121,300] loss:0.075, train acc:0.9869, test acc:0.9350\n",
      "[122,300] loss:0.074, train acc:0.9870, test acc:0.9345\n",
      "[123,300] loss:0.073, train acc:0.9871, test acc:0.9345\n",
      "[124,300] loss:0.072, train acc:0.9874, test acc:0.9345\n",
      "[125,300] loss:0.071, train acc:0.9878, test acc:0.9345\n",
      "[126,300] loss:0.070, train acc:0.9881, test acc:0.9350\n",
      "[127,300] loss:0.069, train acc:0.9883, test acc:0.9350\n",
      "[128,300] loss:0.068, train acc:0.9885, test acc:0.9355\n",
      "[129,300] loss:0.068, train acc:0.9887, test acc:0.9355\n",
      "[130,300] loss:0.067, train acc:0.9893, test acc:0.9355\n",
      "[131,300] loss:0.066, train acc:0.9897, test acc:0.9360\n",
      "[132,300] loss:0.065, train acc:0.9898, test acc:0.9365\n",
      "[133,300] loss:0.064, train acc:0.9900, test acc:0.9360\n",
      "[134,300] loss:0.063, train acc:0.9899, test acc:0.9360\n",
      "[135,300] loss:0.063, train acc:0.9901, test acc:0.9360\n",
      "[136,300] loss:0.062, train acc:0.9904, test acc:0.9360\n",
      "[137,300] loss:0.061, train acc:0.9904, test acc:0.9365\n",
      "[138,300] loss:0.060, train acc:0.9906, test acc:0.9360\n",
      "[139,300] loss:0.060, train acc:0.9907, test acc:0.9360\n",
      "[140,300] loss:0.059, train acc:0.9910, test acc:0.9365\n",
      "[141,300] loss:0.058, train acc:0.9912, test acc:0.9365\n",
      "[142,300] loss:0.057, train acc:0.9913, test acc:0.9360\n",
      "[143,300] loss:0.057, train acc:0.9916, test acc:0.9360\n",
      "[144,300] loss:0.056, train acc:0.9919, test acc:0.9360\n",
      "[145,300] loss:0.055, train acc:0.9923, test acc:0.9365\n",
      "[146,300] loss:0.054, train acc:0.9924, test acc:0.9365\n",
      "[147,300] loss:0.054, train acc:0.9928, test acc:0.9365\n",
      "[148,300] loss:0.053, train acc:0.9931, test acc:0.9365\n",
      "[149,300] loss:0.052, train acc:0.9931, test acc:0.9365\n",
      "[150,300] loss:0.052, train acc:0.9932, test acc:0.9365\n",
      "[151,300] loss:0.051, train acc:0.9935, test acc:0.9365\n",
      "[152,300] loss:0.050, train acc:0.9936, test acc:0.9365\n",
      "[153,300] loss:0.050, train acc:0.9938, test acc:0.9365\n",
      "[154,300] loss:0.049, train acc:0.9939, test acc:0.9365\n",
      "[155,300] loss:0.048, train acc:0.9943, test acc:0.9365\n",
      "[156,300] loss:0.048, train acc:0.9945, test acc:0.9365\n",
      "[157,300] loss:0.047, train acc:0.9949, test acc:0.9365\n",
      "[158,300] loss:0.046, train acc:0.9951, test acc:0.9360\n",
      "[159,300] loss:0.046, train acc:0.9952, test acc:0.9360\n",
      "[160,300] loss:0.045, train acc:0.9952, test acc:0.9360\n",
      "[161,300] loss:0.044, train acc:0.9953, test acc:0.9370\n",
      "[162,300] loss:0.044, train acc:0.9954, test acc:0.9365\n",
      "[163,300] loss:0.043, train acc:0.9954, test acc:0.9360\n",
      "[164,300] loss:0.043, train acc:0.9954, test acc:0.9365\n",
      "[165,300] loss:0.042, train acc:0.9955, test acc:0.9360\n",
      "[166,300] loss:0.042, train acc:0.9958, test acc:0.9360\n",
      "[167,300] loss:0.041, train acc:0.9959, test acc:0.9365\n",
      "[168,300] loss:0.040, train acc:0.9961, test acc:0.9360\n",
      "[169,300] loss:0.040, train acc:0.9962, test acc:0.9360\n",
      "[170,300] loss:0.039, train acc:0.9962, test acc:0.9360\n",
      "[171,300] loss:0.039, train acc:0.9963, test acc:0.9360\n",
      "[172,300] loss:0.038, train acc:0.9965, test acc:0.9365\n",
      "[173,300] loss:0.038, train acc:0.9967, test acc:0.9360\n",
      "[174,300] loss:0.037, train acc:0.9967, test acc:0.9360\n",
      "[175,300] loss:0.037, train acc:0.9968, test acc:0.9360\n",
      "[176,300] loss:0.036, train acc:0.9972, test acc:0.9360\n",
      "[177,300] loss:0.036, train acc:0.9972, test acc:0.9365\n",
      "[178,300] loss:0.035, train acc:0.9974, test acc:0.9360\n",
      "[179,300] loss:0.035, train acc:0.9974, test acc:0.9360\n",
      "[180,300] loss:0.034, train acc:0.9975, test acc:0.9365\n",
      "[181,300] loss:0.034, train acc:0.9976, test acc:0.9360\n",
      "[182,300] loss:0.033, train acc:0.9980, test acc:0.9365\n",
      "[183,300] loss:0.033, train acc:0.9982, test acc:0.9370\n",
      "[184,300] loss:0.032, train acc:0.9983, test acc:0.9365\n",
      "[185,300] loss:0.032, train acc:0.9983, test acc:0.9370\n",
      "[186,300] loss:0.031, train acc:0.9983, test acc:0.9365\n",
      "[187,300] loss:0.031, train acc:0.9983, test acc:0.9370\n",
      "[188,300] loss:0.030, train acc:0.9985, test acc:0.9370\n",
      "[189,300] loss:0.030, train acc:0.9985, test acc:0.9370\n",
      "[190,300] loss:0.029, train acc:0.9985, test acc:0.9375\n",
      "[191,300] loss:0.029, train acc:0.9985, test acc:0.9380\n",
      "[192,300] loss:0.028, train acc:0.9985, test acc:0.9375\n",
      "[193,300] loss:0.028, train acc:0.9986, test acc:0.9370\n",
      "[194,300] loss:0.028, train acc:0.9986, test acc:0.9380\n",
      "[195,300] loss:0.027, train acc:0.9986, test acc:0.9375\n",
      "[196,300] loss:0.027, train acc:0.9986, test acc:0.9375\n",
      "[197,300] loss:0.026, train acc:0.9987, test acc:0.9375\n",
      "[198,300] loss:0.026, train acc:0.9987, test acc:0.9375\n",
      "[199,300] loss:0.026, train acc:0.9987, test acc:0.9385\n",
      "[200,300] loss:0.025, train acc:0.9988, test acc:0.9380\n",
      "[201,300] loss:0.025, train acc:0.9988, test acc:0.9380\n",
      "[202,300] loss:0.024, train acc:0.9988, test acc:0.9380\n",
      "[203,300] loss:0.024, train acc:0.9988, test acc:0.9385\n",
      "[204,300] loss:0.024, train acc:0.9989, test acc:0.9375\n",
      "[205,300] loss:0.023, train acc:0.9990, test acc:0.9380\n",
      "[206,300] loss:0.023, train acc:0.9990, test acc:0.9380\n",
      "[207,300] loss:0.023, train acc:0.9990, test acc:0.9375\n",
      "[208,300] loss:0.022, train acc:0.9990, test acc:0.9385\n",
      "[209,300] loss:0.022, train acc:0.9990, test acc:0.9375\n",
      "[210,300] loss:0.021, train acc:0.9991, test acc:0.9375\n",
      "[211,300] loss:0.021, train acc:0.9991, test acc:0.9375\n",
      "[212,300] loss:0.021, train acc:0.9991, test acc:0.9380\n",
      "[213,300] loss:0.020, train acc:0.9991, test acc:0.9375\n",
      "[214,300] loss:0.020, train acc:0.9991, test acc:0.9375\n",
      "[215,300] loss:0.020, train acc:0.9991, test acc:0.9380\n",
      "[216,300] loss:0.019, train acc:0.9991, test acc:0.9375\n",
      "[217,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[218,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[219,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[220,300] loss:0.018, train acc:0.9992, test acc:0.9375\n",
      "[221,300] loss:0.018, train acc:0.9993, test acc:0.9375\n",
      "[222,300] loss:0.018, train acc:0.9993, test acc:0.9375\n",
      "[223,300] loss:0.017, train acc:0.9994, test acc:0.9375\n",
      "[224,300] loss:0.017, train acc:0.9995, test acc:0.9375\n",
      "[225,300] loss:0.017, train acc:0.9995, test acc:0.9380\n",
      "[226,300] loss:0.016, train acc:0.9995, test acc:0.9375\n",
      "[227,300] loss:0.016, train acc:0.9995, test acc:0.9375\n",
      "[228,300] loss:0.016, train acc:0.9996, test acc:0.9375\n",
      "[229,300] loss:0.016, train acc:0.9996, test acc:0.9375\n",
      "[230,300] loss:0.015, train acc:0.9996, test acc:0.9375\n",
      "[231,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[232,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[233,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[234,300] loss:0.014, train acc:0.9997, test acc:0.9380\n",
      "[235,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[236,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[237,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[238,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[239,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[240,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[241,300] loss:0.013, train acc:0.9998, test acc:0.9370\n",
      "[242,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[243,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[244,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[245,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[246,300] loss:0.012, train acc:0.9999, test acc:0.9375\n",
      "[247,300] loss:0.011, train acc:0.9999, test acc:0.9370\n",
      "[248,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[249,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[250,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[251,300] loss:0.011, train acc:1.0000, test acc:0.9370\n",
      "[252,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[253,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[254,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[255,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[256,300] loss:0.010, train acc:1.0000, test acc:0.9380\n",
      "[257,300] loss:0.010, train acc:1.0000, test acc:0.9380\n",
      "[258,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[259,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[260,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[261,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[262,300] loss:0.009, train acc:1.0000, test acc:0.9375\n",
      "[263,300] loss:0.009, train acc:1.0000, test acc:0.9370\n",
      "[264,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[265,300] loss:0.008, train acc:1.0000, test acc:0.9365\n",
      "[266,300] loss:0.008, train acc:1.0000, test acc:0.9375\n",
      "[267,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[268,300] loss:0.008, train acc:1.0000, test acc:0.9365\n",
      "[269,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[270,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[271,300] loss:0.007, train acc:1.0000, test acc:0.9355\n",
      "[272,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[273,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[274,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[275,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[276,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[277,300] loss:0.007, train acc:1.0000, test acc:0.9365\n",
      "[278,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[279,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[280,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[281,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[282,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[283,300] loss:0.006, train acc:1.0000, test acc:0.9365\n",
      "[284,300] loss:0.006, train acc:1.0000, test acc:0.9365\n",
      "[285,300] loss:0.006, train acc:1.0000, test acc:0.9350\n",
      "[286,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[287,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[288,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[289,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[290,300] loss:0.005, train acc:1.0000, test acc:0.9350\n",
      "[291,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[292,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[293,300] loss:0.005, train acc:1.0000, test acc:0.9350\n",
      "[294,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[295,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[296,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[297,300] loss:0.004, train acc:1.0000, test acc:0.9355\n",
      "[298,300] loss:0.004, train acc:1.0000, test acc:0.9345\n",
      "[299,300] loss:0.004, train acc:1.0000, test acc:0.9350\n",
      "[300,300] loss:0.004, train acc:1.0000, test acc:0.9355\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import train_mlp_readout\n",
    "config.epoch = 300\n",
    "mlp = MLP(2*config.N_hid, config.mlp_hid, config.N_out).to(model.device)\n",
    "train_score, test_score, = train_mlp_readout(model=mlp, \n",
    "                                            config=config,\n",
    "                                            X_train=train_rs,\n",
    "                                            X_test=test_rs,\n",
    "                                            y_train=train_label,\n",
    "                                            y_test=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = train_rs[index][:,1:,0:config.N_hid]\n",
    "# v = v.transpose(1,2).view(-1,30)\n",
    "# node_feat = Egcn(g, v[0:config.N_hid], edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 4, 7, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[index].view(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 200])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v = v.transpose(1,2).reshape(-1, 30)\n",
    "batch_g = dgl.batch([g]*5)\n",
    "batch_edge_attr = torch.cat(([edge_attr]*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = Egcn(batch_g, batch_v, batch_edge_attr)\n",
    "# pred = node_feat.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\44670\\Documents\\GitHub\\Reservoir-Computing\\learn_dgl.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_label' is not defined"
     ]
    }
   ],
   "source": [
    "batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 5, 5, 9, 9, 9, 8, 8, 9])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i in transformer_encoder.named_parameters():\n",
    "    print(i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68160"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*30+64+64*32*32+310+330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\snn\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:447: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rst = self._activation(rst)\n"
     ]
    }
   ],
   "source": [
    "u = [0,1,2,3,2,5]\n",
    "v = [1,2,3,4,0,3]\n",
    "g = dgl.graph((u,v))\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "batch_g = dgl.batch([g, g])\n",
    "feat = torch.ones(6, 3)\n",
    "batch_f = torch.cat([feat, feat])\n",
    "\n",
    "edge_weight = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.1, 1, 1, 1, 1, 1, 1])\n",
    "norm = EdgeWeightNorm(norm='both')\n",
    "norm_edge_weight = norm(g, edge_weight)\n",
    "conv1 = GraphConv(3, 2, norm='none', weight=True, bias=True, activation=None)\n",
    "conv2 = GraphConv(3, 2, norm='none', weight=True, bias=True, activation=nn.Softmax())\n",
    "conv2.weight = conv1.weight\n",
    "conv2.bias = conv1.bias\n",
    "\n",
    "# out1 = conv(batch_g, batch_f, edge_weight=norm_edge_weight)\n",
    "out2 = conv1(batch_g, batch_f, edge_weight=None)\n",
    "out3 = conv2(batch_g, batch_f, edge_weight=None)\n",
    "# out3 = conv(g, feat, edge_weight=edge_weight)\n",
    "# print(out1,'\\n', out2, '\\n', out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-4.7213,  1.2245],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-1.5738,  0.4082],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-4.7213,  1.2245],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-1.5738,  0.4082]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0026, 0.9974],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.1211, 0.8789],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0026, 0.9974],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.1211, 0.8789]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2,out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\GitHubClone\\Reservoir-Computing\\learn_dgl.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/GitHubClone/Reservoir-Computing/learn_dgl.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39madjacency_matrix()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/GitHubClone/Reservoir-Computing/learn_dgl.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first."
     ]
    }
   ],
   "source": [
    "a = graph.adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 15]), torch.Size([30, 3, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes, num_edges = 8, 30\n",
    "node_dim = 20\n",
    "edge_dim = 1\n",
    "graph = dgl.rand_graph(num_nodes, num_edges)\n",
    "node_feats = torch.rand((num_nodes, node_dim))\n",
    "edge_feats = torch.rand((num_edges, edge_dim))\n",
    "egat = EGATConv(in_node_feats=node_dim,\n",
    "                in_edge_feats=edge_dim,\n",
    "                out_node_feats=15,\n",
    "                out_edge_feats=1,\n",
    "                num_heads=3)\n",
    "#forward pass\n",
    "new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "new_node_feats.shape, new_edge_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7398],\n",
       "        [2.1219],\n",
       "        [0.2601]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edge_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3739], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edge_feats.mean(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Number of categories: 7\n",
      "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset('./data')\n",
    "print('Number of categories:', dataset.num_classes)\n",
    "g = dataset[0]\n",
    "print(g.ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\44670\\Documents\\GitHub\\Reservoir-Computing\\learn_dgl.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mif\u001b[39;00m e \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIn epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, val acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m (best \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m), test acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m (best \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                 e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m model \u001b[39m=\u001b[39m GCN(g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m16\u001b[39m, dataset\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m train(g, model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GCN' is not defined"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(30):\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(1)\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 101, 200])\n"
     ]
    }
   ],
   "source": [
    "from config import Config as config\n",
    "from RC import torchRC\n",
    "model = torchRC(config)\n",
    "mem, spike = model(torch.rand(16, config.frames, 50))\n",
    "print(mem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.reservoir.A.numpy()\n",
    "edge_index = torch.tensor(np.where(A!=0), dtype=torch.long)\n",
    "edge_attr = torch.tensor(np.array([A[i,j] for i,j in edge_index.T]))\n",
    "u = edge_index[0]\n",
    "v = edge_index[1]\n",
    "mems = mem[0,0,1:].T\n",
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = mems\n",
    "g.edata['w'] = edge_attr\n",
    "\n",
    "h = dgl.graph((u, v))\n",
    "h.ndata['x'] = mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = mems\n",
    "g.edata['w'] = edge_attr\n",
    "\n",
    "h = dgl.graph((u, v))\n",
    "h.ndata['x'] = mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "conv1 = GraphConv(config.frames, 16)\n",
    "out = conv1(g, mems)\n",
    "out_h = conv1(h, mems)\n",
    "print(out_h[0])\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3371, -0.0486,  0.0422,  ...,  0.0489, -0.0459,  0.3718])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "conv2 = GCNConv(100, 16)\n",
    "conv2(mems, edge_index, edge_attr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71755e07d76f363f96e0c11ca35acb8f13d731f42c5e22d3b26f54f837a1c4d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
