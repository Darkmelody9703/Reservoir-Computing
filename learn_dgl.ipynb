{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl.nn import EGATConv, EdgeWeightNorm, GraphConv\n",
    "from dgl.utils import expand_as_pair\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    transformer encoder for reservoir readout layer\n",
    "    '''\n",
    "    def __init__(self, config:Config) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, \n",
    "                                                   nhead=config.n_heads,\n",
    "                                                   dim_feedforward=config.d_ff,)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.encoder_layer)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(config.N_hid, config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input x shape = [batch, channel, N_hid]\n",
    "        channel = config.frames\n",
    "        '''\n",
    "        x = self.fc(x)                  # out shape [batch, channel, d_model]\n",
    "        x = self.transformer_encoder(x) # out shape [batch, channel, d_model]\n",
    "        x = self.avgpool(x)             # out shape [batch, channel, 1]\n",
    "        x = torch.flatten(x, 1)         # out shape [batch, channel]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = Config()\n",
    "configuration.d_model = 64\n",
    "configuration.encoder_layer = 2\n",
    "configuration.n_heads = 4\n",
    "configuration.d_ff = 64\n",
    "\n",
    "configuration.lr = 1e-3\n",
    "configuration.epoch = 3000\n",
    "model = Transformer(configuration)\n",
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_transformer_readout(model:Transformer,\n",
    "                      config:Config,\n",
    "                      X_train,\n",
    "                      X_test,\n",
    "                      y_train,\n",
    "                      y_test,\n",
    "                      ):\n",
    "    train_num = X_train.shape[0]\n",
    "    test_num = X_test.shape[0]\n",
    "    iteration = int(train_num/config.batch_size)\n",
    "    iter = int(test_num/config.batch_size)\n",
    "    cost = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    for epoch in range(config.epoch) :\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        train_correct = 0\n",
    "        for i in range(iteration):\n",
    "            x = X_train[i*config.batch_size:(i+1)*config.batch_size] # [batch, config.frames, config.N_hid]\n",
    "            y = y_train[i*config.batch_size:(i+1)*config.batch_size]\n",
    "            print(y.shape)\n",
    "            out = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = cost(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            _, id = torch.max(out.data, 1)\n",
    "            sum_loss += loss.data\n",
    "            train_correct+=torch.sum(id==y.data)\n",
    "        \n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        for i in range(iter):\n",
    "            x = X_test[i*config.batch_size:(i+1)*config.batch_size]\n",
    "            y = y_test[i*config.batch_size:(i+1)*config.batch_size]\n",
    "            outputs = model(x)\n",
    "            _, id = torch.max(outputs.data, 1)\n",
    "            test_correct += torch.sum(id == y.data)\n",
    "        if config.verbose:\n",
    "            print('[%d,%d] loss:%.03f, train acc:%.4f, test acc:%.4f' % (epoch+1, config.epoch, sum_loss/iteration, train_correct/train_num, test_correct/test_num))\n",
    "        \n",
    "    return train_correct / train_num, test_correct / test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,300] loss:3.392, train acc:0.1160, test acc:0.1140\n",
      "[2,300] loss:3.365, train acc:0.1150, test acc:0.1050\n",
      "[3,300] loss:3.341, train acc:0.1000, test acc:0.1180\n",
      "[4,300] loss:3.314, train acc:0.0960, test acc:0.1250\n",
      "[5,300] loss:3.297, train acc:0.1040, test acc:0.0970\n",
      "[6,300] loss:3.265, train acc:0.1340, test acc:0.1180\n",
      "[7,300] loss:3.242, train acc:0.1130, test acc:0.0910\n",
      "[8,300] loss:3.213, train acc:0.1450, test acc:0.1030\n",
      "[9,300] loss:3.189, train acc:0.1260, test acc:0.0860\n",
      "[10,300] loss:3.164, train acc:0.1240, test acc:0.1190\n",
      "[11,300] loss:3.147, train acc:0.1270, test acc:0.1070\n",
      "[12,300] loss:3.114, train acc:0.1190, test acc:0.0980\n",
      "[13,300] loss:3.121, train acc:0.1080, test acc:0.1040\n",
      "[14,300] loss:3.078, train acc:0.1270, test acc:0.1130\n",
      "[15,300] loss:3.052, train acc:0.1240, test acc:0.1010\n",
      "[16,300] loss:3.033, train acc:0.1240, test acc:0.1360\n",
      "[17,300] loss:3.022, train acc:0.1230, test acc:0.0790\n",
      "[18,300] loss:3.032, train acc:0.1430, test acc:0.1240\n",
      "[19,300] loss:3.021, train acc:0.1130, test acc:0.1360\n",
      "[20,300] loss:3.130, train acc:0.1130, test acc:0.1420\n",
      "[21,300] loss:3.031, train acc:0.1240, test acc:0.0890\n",
      "[22,300] loss:3.028, train acc:0.1100, test acc:0.1090\n",
      "[23,300] loss:2.965, train acc:0.1130, test acc:0.1200\n",
      "[24,300] loss:2.948, train acc:0.0960, test acc:0.1040\n",
      "[25,300] loss:2.924, train acc:0.1020, test acc:0.0850\n",
      "[26,300] loss:2.907, train acc:0.1110, test acc:0.1230\n",
      "[27,300] loss:2.897, train acc:0.1160, test acc:0.1260\n",
      "[28,300] loss:2.857, train acc:0.1250, test acc:0.1160\n",
      "[29,300] loss:2.891, train acc:0.1080, test acc:0.0800\n",
      "[30,300] loss:2.826, train acc:0.1140, test acc:0.1060\n",
      "[31,300] loss:2.840, train acc:0.1330, test acc:0.1020\n",
      "[32,300] loss:2.839, train acc:0.1020, test acc:0.0930\n",
      "[33,300] loss:2.794, train acc:0.0920, test acc:0.1000\n",
      "[34,300] loss:2.895, train acc:0.1120, test acc:0.0920\n",
      "[35,300] loss:2.802, train acc:0.1120, test acc:0.1230\n",
      "[36,300] loss:2.777, train acc:0.1350, test acc:0.1360\n",
      "[37,300] loss:2.765, train acc:0.1050, test acc:0.1210\n",
      "[38,300] loss:2.742, train acc:0.1260, test acc:0.0990\n",
      "[39,300] loss:2.725, train acc:0.1070, test acc:0.1250\n",
      "[40,300] loss:2.716, train acc:0.1090, test acc:0.1200\n",
      "[41,300] loss:2.714, train acc:0.1100, test acc:0.1250\n",
      "[42,300] loss:2.713, train acc:0.1080, test acc:0.0930\n",
      "[43,300] loss:2.775, train acc:0.0990, test acc:0.1320\n",
      "[44,300] loss:2.732, train acc:0.1010, test acc:0.0880\n",
      "[45,300] loss:2.669, train acc:0.1200, test acc:0.1270\n",
      "[46,300] loss:2.696, train acc:0.1110, test acc:0.0970\n",
      "[47,300] loss:2.695, train acc:0.1170, test acc:0.1270\n",
      "[48,300] loss:2.649, train acc:0.1340, test acc:0.1470\n",
      "[49,300] loss:2.683, train acc:0.1240, test acc:0.0690\n",
      "[50,300] loss:2.720, train acc:0.1140, test acc:0.1370\n",
      "[51,300] loss:2.815, train acc:0.0980, test acc:0.0840\n",
      "[52,300] loss:2.662, train acc:0.1150, test acc:0.1030\n",
      "[53,300] loss:2.615, train acc:0.1230, test acc:0.1300\n",
      "[54,300] loss:2.596, train acc:0.1110, test acc:0.1420\n",
      "[55,300] loss:2.802, train acc:0.1070, test acc:0.0970\n",
      "[56,300] loss:3.005, train acc:0.0860, test acc:0.0920\n",
      "[57,300] loss:2.691, train acc:0.0940, test acc:0.1090\n",
      "[58,300] loss:2.651, train acc:0.1200, test acc:0.1050\n",
      "[59,300] loss:2.589, train acc:0.0990, test acc:0.1310\n",
      "[60,300] loss:2.612, train acc:0.1150, test acc:0.0950\n",
      "[61,300] loss:2.571, train acc:0.0930, test acc:0.0990\n",
      "[62,300] loss:2.549, train acc:0.1230, test acc:0.1580\n",
      "[63,300] loss:2.529, train acc:0.1300, test acc:0.1420\n",
      "[64,300] loss:2.515, train acc:0.1200, test acc:0.1410\n",
      "[65,300] loss:2.531, train acc:0.1230, test acc:0.1340\n",
      "[66,300] loss:2.513, train acc:0.1280, test acc:0.1250\n",
      "[67,300] loss:2.487, train acc:0.1190, test acc:0.1600\n",
      "[68,300] loss:2.488, train acc:0.1130, test acc:0.1270\n",
      "[69,300] loss:2.499, train acc:0.1510, test acc:0.1340\n",
      "[70,300] loss:2.457, train acc:0.1400, test acc:0.1200\n",
      "[71,300] loss:2.462, train acc:0.1550, test acc:0.1050\n",
      "[72,300] loss:2.433, train acc:0.1520, test acc:0.1290\n",
      "[73,300] loss:2.408, train acc:0.1290, test acc:0.1280\n",
      "[74,300] loss:2.451, train acc:0.1370, test acc:0.0990\n",
      "[75,300] loss:2.458, train acc:0.1270, test acc:0.1360\n",
      "[76,300] loss:2.409, train acc:0.1490, test acc:0.1480\n",
      "[77,300] loss:2.389, train acc:0.1610, test acc:0.1250\n",
      "[78,300] loss:2.394, train acc:0.1560, test acc:0.1330\n",
      "[79,300] loss:2.367, train acc:0.1420, test acc:0.1490\n",
      "[80,300] loss:2.341, train acc:0.1630, test acc:0.1220\n",
      "[81,300] loss:2.346, train acc:0.1570, test acc:0.1200\n",
      "[82,300] loss:2.340, train acc:0.1560, test acc:0.1490\n",
      "[83,300] loss:2.309, train acc:0.1840, test acc:0.1420\n",
      "[84,300] loss:2.317, train acc:0.1730, test acc:0.1310\n",
      "[85,300] loss:2.279, train acc:0.1730, test acc:0.1620\n",
      "[86,300] loss:2.301, train acc:0.1780, test acc:0.1550\n",
      "[87,300] loss:2.285, train acc:0.1640, test acc:0.1820\n",
      "[88,300] loss:2.306, train acc:0.1670, test acc:0.1660\n",
      "[89,300] loss:2.249, train acc:0.1900, test acc:0.1890\n",
      "[90,300] loss:2.210, train acc:0.1960, test acc:0.1780\n",
      "[91,300] loss:2.212, train acc:0.1970, test acc:0.1590\n",
      "[92,300] loss:2.218, train acc:0.2030, test acc:0.2200\n",
      "[93,300] loss:2.195, train acc:0.1920, test acc:0.1570\n",
      "[94,300] loss:2.186, train acc:0.2020, test acc:0.1850\n",
      "[95,300] loss:2.174, train acc:0.2250, test acc:0.1720\n",
      "[96,300] loss:2.214, train acc:0.2150, test acc:0.1680\n",
      "[97,300] loss:2.173, train acc:0.2560, test acc:0.1890\n",
      "[98,300] loss:2.157, train acc:0.2680, test acc:0.1630\n",
      "[99,300] loss:2.154, train acc:0.2630, test acc:0.2050\n",
      "[100,300] loss:2.123, train acc:0.3130, test acc:0.1840\n",
      "[101,300] loss:2.089, train acc:0.2800, test acc:0.2500\n",
      "[102,300] loss:1.998, train acc:0.3310, test acc:0.2610\n",
      "[103,300] loss:1.967, train acc:0.3090, test acc:0.2930\n",
      "[104,300] loss:1.990, train acc:0.3160, test acc:0.2780\n",
      "[105,300] loss:2.003, train acc:0.2940, test acc:0.2570\n",
      "[106,300] loss:1.967, train acc:0.3110, test acc:0.2840\n",
      "[107,300] loss:1.907, train acc:0.3540, test acc:0.3300\n",
      "[108,300] loss:1.842, train acc:0.3600, test acc:0.2940\n",
      "[109,300] loss:1.797, train acc:0.3850, test acc:0.2910\n",
      "[110,300] loss:1.815, train acc:0.3720, test acc:0.3530\n",
      "[111,300] loss:1.754, train acc:0.4000, test acc:0.3240\n",
      "[112,300] loss:1.779, train acc:0.4230, test acc:0.3400\n",
      "[113,300] loss:1.738, train acc:0.3930, test acc:0.3350\n",
      "[114,300] loss:1.725, train acc:0.3980, test acc:0.3320\n",
      "[115,300] loss:1.681, train acc:0.4280, test acc:0.3520\n",
      "[116,300] loss:1.670, train acc:0.4260, test acc:0.3310\n",
      "[117,300] loss:1.599, train acc:0.4520, test acc:0.3650\n",
      "[118,300] loss:1.567, train acc:0.4440, test acc:0.3920\n",
      "[119,300] loss:1.606, train acc:0.4670, test acc:0.3760\n",
      "[120,300] loss:1.624, train acc:0.4510, test acc:0.3410\n",
      "[121,300] loss:1.549, train acc:0.4810, test acc:0.3700\n",
      "[122,300] loss:1.559, train acc:0.4720, test acc:0.3480\n",
      "[123,300] loss:1.526, train acc:0.4800, test acc:0.3790\n",
      "[124,300] loss:1.458, train acc:0.5230, test acc:0.3650\n",
      "[125,300] loss:1.527, train acc:0.4950, test acc:0.3700\n",
      "[126,300] loss:1.536, train acc:0.5210, test acc:0.3210\n",
      "[127,300] loss:1.551, train acc:0.4890, test acc:0.3790\n",
      "[128,300] loss:1.530, train acc:0.4930, test acc:0.3980\n",
      "[129,300] loss:1.485, train acc:0.4870, test acc:0.3340\n",
      "[130,300] loss:1.479, train acc:0.4930, test acc:0.4080\n",
      "[131,300] loss:1.361, train acc:0.5340, test acc:0.4130\n",
      "[132,300] loss:1.359, train acc:0.5720, test acc:0.4270\n",
      "[133,300] loss:1.391, train acc:0.5430, test acc:0.4230\n",
      "[134,300] loss:1.350, train acc:0.5620, test acc:0.4120\n",
      "[135,300] loss:1.329, train acc:0.5450, test acc:0.4340\n",
      "[136,300] loss:1.257, train acc:0.6090, test acc:0.3940\n",
      "[137,300] loss:1.303, train acc:0.5690, test acc:0.4300\n",
      "[138,300] loss:1.350, train acc:0.5740, test acc:0.4150\n",
      "[139,300] loss:1.437, train acc:0.4930, test acc:0.3920\n",
      "[140,300] loss:1.264, train acc:0.6160, test acc:0.4290\n",
      "[141,300] loss:1.184, train acc:0.6200, test acc:0.3950\n",
      "[142,300] loss:1.125, train acc:0.6380, test acc:0.4440\n",
      "[143,300] loss:1.129, train acc:0.6400, test acc:0.4130\n",
      "[144,300] loss:1.089, train acc:0.6580, test acc:0.4210\n",
      "[145,300] loss:1.050, train acc:0.6610, test acc:0.4230\n",
      "[146,300] loss:1.043, train acc:0.6710, test acc:0.4420\n",
      "[147,300] loss:1.084, train acc:0.6600, test acc:0.4230\n",
      "[148,300] loss:1.026, train acc:0.6690, test acc:0.4500\n",
      "[149,300] loss:1.008, train acc:0.6850, test acc:0.4290\n",
      "[150,300] loss:0.987, train acc:0.6890, test acc:0.4110\n",
      "[151,300] loss:1.041, train acc:0.6530, test acc:0.4460\n",
      "[152,300] loss:1.180, train acc:0.6500, test acc:0.3810\n",
      "[153,300] loss:1.101, train acc:0.6330, test acc:0.4470\n",
      "[154,300] loss:1.126, train acc:0.6300, test acc:0.4430\n",
      "[155,300] loss:1.118, train acc:0.6140, test acc:0.4170\n",
      "[156,300] loss:1.082, train acc:0.6090, test acc:0.4660\n",
      "[157,300] loss:1.058, train acc:0.6210, test acc:0.4110\n",
      "[158,300] loss:1.060, train acc:0.6430, test acc:0.4560\n",
      "[159,300] loss:0.948, train acc:0.6790, test acc:0.4400\n",
      "[160,300] loss:0.930, train acc:0.6810, test acc:0.4680\n",
      "[161,300] loss:0.849, train acc:0.7180, test acc:0.4360\n",
      "[162,300] loss:0.905, train acc:0.6820, test acc:0.4610\n",
      "[163,300] loss:0.863, train acc:0.7200, test acc:0.4540\n",
      "[164,300] loss:0.806, train acc:0.7440, test acc:0.4630\n",
      "[165,300] loss:0.820, train acc:0.7290, test acc:0.4650\n",
      "[166,300] loss:0.775, train acc:0.7580, test acc:0.4720\n",
      "[167,300] loss:0.871, train acc:0.7310, test acc:0.4150\n",
      "[168,300] loss:1.080, train acc:0.6610, test acc:0.4820\n",
      "[169,300] loss:0.852, train acc:0.7240, test acc:0.4980\n",
      "[170,300] loss:0.804, train acc:0.7380, test acc:0.4490\n",
      "[171,300] loss:0.797, train acc:0.7470, test acc:0.4550\n",
      "[172,300] loss:0.808, train acc:0.7410, test acc:0.4440\n",
      "[173,300] loss:0.766, train acc:0.7610, test acc:0.4740\n",
      "[174,300] loss:0.749, train acc:0.7720, test acc:0.4960\n",
      "[175,300] loss:0.765, train acc:0.7680, test acc:0.4740\n",
      "[176,300] loss:0.700, train acc:0.8050, test acc:0.4880\n",
      "[177,300] loss:0.689, train acc:0.7870, test acc:0.5110\n",
      "[178,300] loss:0.652, train acc:0.8180, test acc:0.5300\n",
      "[179,300] loss:0.622, train acc:0.8300, test acc:0.4940\n",
      "[180,300] loss:0.689, train acc:0.8040, test acc:0.4810\n",
      "[181,300] loss:0.761, train acc:0.7840, test acc:0.4800\n",
      "[182,300] loss:0.681, train acc:0.8150, test acc:0.4930\n",
      "[183,300] loss:0.576, train acc:0.8550, test acc:0.5240\n",
      "[184,300] loss:0.646, train acc:0.8190, test acc:0.5180\n",
      "[185,300] loss:0.591, train acc:0.8400, test acc:0.5370\n",
      "[186,300] loss:0.571, train acc:0.8500, test acc:0.5090\n",
      "[187,300] loss:0.533, train acc:0.8620, test acc:0.5100\n",
      "[188,300] loss:0.560, train acc:0.8400, test acc:0.4730\n",
      "[189,300] loss:0.562, train acc:0.8490, test acc:0.5070\n",
      "[190,300] loss:0.543, train acc:0.8510, test acc:0.5070\n",
      "[191,300] loss:0.598, train acc:0.8380, test acc:0.5250\n",
      "[192,300] loss:0.518, train acc:0.8690, test acc:0.5000\n",
      "[193,300] loss:0.573, train acc:0.8400, test acc:0.4650\n",
      "[194,300] loss:0.630, train acc:0.8310, test acc:0.4930\n",
      "[195,300] loss:0.531, train acc:0.8480, test acc:0.5540\n",
      "[196,300] loss:0.452, train acc:0.8800, test acc:0.5110\n",
      "[197,300] loss:0.503, train acc:0.8600, test acc:0.5100\n",
      "[198,300] loss:0.427, train acc:0.8880, test acc:0.5590\n",
      "[199,300] loss:0.355, train acc:0.9190, test acc:0.5620\n",
      "[200,300] loss:0.385, train acc:0.9130, test acc:0.5610\n",
      "[201,300] loss:0.360, train acc:0.9210, test acc:0.5570\n",
      "[202,300] loss:0.350, train acc:0.9290, test acc:0.5360\n",
      "[203,300] loss:0.356, train acc:0.9180, test acc:0.5630\n",
      "[204,300] loss:0.332, train acc:0.9300, test acc:0.5410\n",
      "[205,300] loss:0.304, train acc:0.9350, test acc:0.5440\n",
      "[206,300] loss:0.303, train acc:0.9370, test acc:0.5640\n",
      "[207,300] loss:0.315, train acc:0.9290, test acc:0.5820\n",
      "[208,300] loss:0.285, train acc:0.9380, test acc:0.5570\n",
      "[209,300] loss:0.281, train acc:0.9450, test acc:0.5790\n",
      "[210,300] loss:0.271, train acc:0.9450, test acc:0.5680\n",
      "[211,300] loss:0.307, train acc:0.9470, test acc:0.5480\n",
      "[212,300] loss:0.335, train acc:0.9400, test acc:0.5610\n",
      "[213,300] loss:0.300, train acc:0.9470, test acc:0.5750\n",
      "[214,300] loss:0.253, train acc:0.9500, test acc:0.5780\n",
      "[215,300] loss:0.224, train acc:0.9620, test acc:0.5640\n",
      "[216,300] loss:0.241, train acc:0.9650, test acc:0.5520\n",
      "[217,300] loss:0.219, train acc:0.9710, test acc:0.5640\n",
      "[218,300] loss:0.216, train acc:0.9620, test acc:0.5440\n",
      "[219,300] loss:0.274, train acc:0.9510, test acc:0.5770\n",
      "[220,300] loss:0.209, train acc:0.9630, test acc:0.5910\n",
      "[221,300] loss:0.178, train acc:0.9710, test acc:0.6020\n",
      "[222,300] loss:0.194, train acc:0.9690, test acc:0.6010\n",
      "[223,300] loss:0.187, train acc:0.9710, test acc:0.5800\n",
      "[224,300] loss:0.170, train acc:0.9700, test acc:0.5760\n",
      "[225,300] loss:0.158, train acc:0.9700, test acc:0.5740\n",
      "[226,300] loss:0.158, train acc:0.9740, test acc:0.5800\n",
      "[227,300] loss:0.156, train acc:0.9700, test acc:0.5950\n",
      "[228,300] loss:0.140, train acc:0.9810, test acc:0.5970\n",
      "[229,300] loss:0.137, train acc:0.9770, test acc:0.6040\n",
      "[230,300] loss:0.125, train acc:0.9850, test acc:0.5840\n",
      "[231,300] loss:0.123, train acc:0.9810, test acc:0.5890\n",
      "[232,300] loss:0.115, train acc:0.9840, test acc:0.6010\n",
      "[233,300] loss:0.113, train acc:0.9810, test acc:0.6010\n",
      "[234,300] loss:0.110, train acc:0.9810, test acc:0.5860\n",
      "[235,300] loss:0.116, train acc:0.9820, test acc:0.5920\n",
      "[236,300] loss:0.103, train acc:0.9810, test acc:0.5950\n",
      "[237,300] loss:0.098, train acc:0.9860, test acc:0.5720\n",
      "[238,300] loss:0.106, train acc:0.9850, test acc:0.5930\n",
      "[239,300] loss:0.096, train acc:0.9880, test acc:0.5920\n",
      "[240,300] loss:0.097, train acc:0.9880, test acc:0.5920\n",
      "[241,300] loss:0.089, train acc:0.9850, test acc:0.5950\n",
      "[242,300] loss:0.088, train acc:0.9880, test acc:0.5960\n",
      "[243,300] loss:0.091, train acc:0.9860, test acc:0.6070\n",
      "[244,300] loss:0.088, train acc:0.9870, test acc:0.5840\n",
      "[245,300] loss:0.094, train acc:0.9870, test acc:0.5870\n",
      "[246,300] loss:0.116, train acc:0.9840, test acc:0.5990\n",
      "[247,300] loss:0.121, train acc:0.9790, test acc:0.6090\n",
      "[248,300] loss:0.109, train acc:0.9870, test acc:0.6010\n",
      "[249,300] loss:0.120, train acc:0.9820, test acc:0.6030\n",
      "[250,300] loss:0.150, train acc:0.9750, test acc:0.5950\n",
      "[251,300] loss:0.181, train acc:0.9600, test acc:0.5580\n",
      "[252,300] loss:0.207, train acc:0.9630, test acc:0.5980\n",
      "[253,300] loss:0.159, train acc:0.9790, test acc:0.5870\n",
      "[254,300] loss:0.163, train acc:0.9710, test acc:0.5820\n",
      "[255,300] loss:0.135, train acc:0.9800, test acc:0.5730\n",
      "[256,300] loss:0.114, train acc:0.9800, test acc:0.5750\n",
      "[257,300] loss:0.148, train acc:0.9800, test acc:0.5670\n",
      "[258,300] loss:0.126, train acc:0.9830, test acc:0.6120\n",
      "[259,300] loss:0.098, train acc:0.9840, test acc:0.6270\n",
      "[260,300] loss:0.088, train acc:0.9870, test acc:0.5950\n",
      "[261,300] loss:0.095, train acc:0.9850, test acc:0.6220\n",
      "[262,300] loss:0.091, train acc:0.9850, test acc:0.6040\n",
      "[263,300] loss:0.074, train acc:0.9850, test acc:0.6200\n",
      "[264,300] loss:0.069, train acc:0.9910, test acc:0.6110\n",
      "[265,300] loss:0.072, train acc:0.9890, test acc:0.6190\n",
      "[266,300] loss:0.069, train acc:0.9910, test acc:0.6100\n",
      "[267,300] loss:0.065, train acc:0.9930, test acc:0.6090\n",
      "[268,300] loss:0.063, train acc:0.9890, test acc:0.6040\n",
      "[269,300] loss:0.061, train acc:0.9910, test acc:0.6090\n",
      "[270,300] loss:0.058, train acc:0.9890, test acc:0.5990\n",
      "[271,300] loss:0.061, train acc:0.9900, test acc:0.6150\n",
      "[272,300] loss:0.061, train acc:0.9920, test acc:0.6060\n",
      "[273,300] loss:0.067, train acc:0.9910, test acc:0.6090\n",
      "[274,300] loss:0.062, train acc:0.9890, test acc:0.5990\n",
      "[275,300] loss:0.072, train acc:0.9900, test acc:0.6040\n",
      "[276,300] loss:0.062, train acc:0.9910, test acc:0.5960\n",
      "[277,300] loss:0.057, train acc:0.9890, test acc:0.5970\n",
      "[278,300] loss:0.059, train acc:0.9910, test acc:0.6050\n",
      "[279,300] loss:0.062, train acc:0.9900, test acc:0.6130\n",
      "[280,300] loss:0.057, train acc:0.9890, test acc:0.6170\n",
      "[281,300] loss:0.061, train acc:0.9910, test acc:0.6040\n",
      "[282,300] loss:0.067, train acc:0.9890, test acc:0.6090\n",
      "[283,300] loss:0.063, train acc:0.9910, test acc:0.6010\n",
      "[284,300] loss:0.065, train acc:0.9870, test acc:0.6020\n",
      "[285,300] loss:0.070, train acc:0.9920, test acc:0.6000\n",
      "[286,300] loss:0.064, train acc:0.9890, test acc:0.5820\n",
      "[287,300] loss:0.099, train acc:0.9760, test acc:0.5950\n",
      "[288,300] loss:0.073, train acc:0.9900, test acc:0.6090\n",
      "[289,300] loss:0.072, train acc:0.9900, test acc:0.6060\n",
      "[290,300] loss:0.068, train acc:0.9910, test acc:0.6090\n",
      "[291,300] loss:0.054, train acc:0.9900, test acc:0.6030\n",
      "[292,300] loss:0.098, train acc:0.9790, test acc:0.6040\n",
      "[293,300] loss:0.078, train acc:0.9880, test acc:0.5910\n",
      "[294,300] loss:0.065, train acc:0.9900, test acc:0.6090\n",
      "[295,300] loss:0.060, train acc:0.9900, test acc:0.6000\n",
      "[296,300] loss:0.063, train acc:0.9910, test acc:0.5970\n",
      "[297,300] loss:0.053, train acc:0.9920, test acc:0.5950\n",
      "[298,300] loss:0.059, train acc:0.9920, test acc:0.6100\n",
      "[299,300] loss:0.070, train acc:0.9910, test acc:0.6020\n",
      "[300,300] loss:0.052, train acc:0.9910, test acc:0.6050\n",
      "[301,300] loss:0.053, train acc:0.9940, test acc:0.5930\n",
      "[302,300] loss:0.051, train acc:0.9940, test acc:0.6050\n",
      "[303,300] loss:0.047, train acc:0.9930, test acc:0.6130\n",
      "[304,300] loss:0.050, train acc:0.9920, test acc:0.6100\n",
      "[305,300] loss:0.054, train acc:0.9910, test acc:0.6040\n",
      "[306,300] loss:0.052, train acc:0.9950, test acc:0.6040\n",
      "[307,300] loss:0.049, train acc:0.9930, test acc:0.6140\n",
      "[308,300] loss:0.046, train acc:0.9920, test acc:0.6100\n",
      "[309,300] loss:0.054, train acc:0.9920, test acc:0.6190\n",
      "[310,300] loss:0.063, train acc:0.9910, test acc:0.5980\n",
      "[311,300] loss:0.080, train acc:0.9840, test acc:0.6110\n",
      "[312,300] loss:0.066, train acc:0.9900, test acc:0.6200\n",
      "[313,300] loss:0.051, train acc:0.9910, test acc:0.6120\n",
      "[314,300] loss:0.059, train acc:0.9900, test acc:0.6320\n",
      "[315,300] loss:0.047, train acc:0.9940, test acc:0.6240\n",
      "[316,300] loss:0.042, train acc:0.9920, test acc:0.6290\n",
      "[317,300] loss:0.041, train acc:0.9910, test acc:0.6300\n",
      "[318,300] loss:0.039, train acc:0.9950, test acc:0.6270\n",
      "[319,300] loss:0.039, train acc:0.9930, test acc:0.6270\n",
      "[320,300] loss:0.043, train acc:0.9940, test acc:0.6280\n",
      "[321,300] loss:0.042, train acc:0.9930, test acc:0.6120\n",
      "[322,300] loss:0.041, train acc:0.9920, test acc:0.6230\n",
      "[323,300] loss:0.047, train acc:0.9930, test acc:0.6050\n",
      "[324,300] loss:0.057, train acc:0.9910, test acc:0.6040\n",
      "[325,300] loss:0.051, train acc:0.9910, test acc:0.6140\n",
      "[326,300] loss:0.045, train acc:0.9920, test acc:0.6310\n",
      "[327,300] loss:0.037, train acc:0.9920, test acc:0.6290\n",
      "[328,300] loss:0.035, train acc:0.9940, test acc:0.6210\n",
      "[329,300] loss:0.034, train acc:0.9930, test acc:0.6220\n",
      "[330,300] loss:0.034, train acc:0.9940, test acc:0.6310\n",
      "[331,300] loss:0.034, train acc:0.9930, test acc:0.6280\n",
      "[332,300] loss:0.034, train acc:0.9940, test acc:0.6160\n",
      "[333,300] loss:0.033, train acc:0.9920, test acc:0.6260\n",
      "[334,300] loss:0.033, train acc:0.9930, test acc:0.6220\n",
      "[335,300] loss:0.034, train acc:0.9930, test acc:0.6230\n",
      "[336,300] loss:0.043, train acc:0.9940, test acc:0.6040\n",
      "[337,300] loss:0.071, train acc:0.9890, test acc:0.6030\n",
      "[338,300] loss:0.062, train acc:0.9920, test acc:0.5920\n",
      "[339,300] loss:0.068, train acc:0.9900, test acc:0.5730\n",
      "[340,300] loss:0.105, train acc:0.9860, test acc:0.5860\n",
      "[341,300] loss:0.072, train acc:0.9870, test acc:0.6080\n",
      "[342,300] loss:0.085, train acc:0.9890, test acc:0.6150\n",
      "[343,300] loss:0.056, train acc:0.9900, test acc:0.6060\n",
      "[344,300] loss:0.048, train acc:0.9920, test acc:0.5910\n",
      "[345,300] loss:0.050, train acc:0.9930, test acc:0.6190\n",
      "[346,300] loss:0.046, train acc:0.9910, test acc:0.5840\n",
      "[347,300] loss:0.044, train acc:0.9940, test acc:0.6150\n",
      "[348,300] loss:0.044, train acc:0.9910, test acc:0.6130\n",
      "[349,300] loss:0.045, train acc:0.9900, test acc:0.5960\n",
      "[350,300] loss:0.040, train acc:0.9910, test acc:0.6150\n",
      "[351,300] loss:0.046, train acc:0.9910, test acc:0.6200\n",
      "[352,300] loss:0.046, train acc:0.9910, test acc:0.6260\n",
      "[353,300] loss:0.042, train acc:0.9910, test acc:0.6120\n",
      "[354,300] loss:0.090, train acc:0.9800, test acc:0.6020\n",
      "[355,300] loss:0.054, train acc:0.9910, test acc:0.6120\n",
      "[356,300] loss:0.048, train acc:0.9940, test acc:0.6260\n",
      "[357,300] loss:0.062, train acc:0.9880, test acc:0.5780\n",
      "[358,300] loss:0.129, train acc:0.9720, test acc:0.6130\n",
      "[359,300] loss:0.064, train acc:0.9890, test acc:0.6010\n",
      "[360,300] loss:0.043, train acc:0.9930, test acc:0.6240\n",
      "[361,300] loss:0.032, train acc:0.9940, test acc:0.6150\n",
      "[362,300] loss:0.030, train acc:0.9940, test acc:0.6170\n",
      "[363,300] loss:0.035, train acc:0.9930, test acc:0.6160\n",
      "[364,300] loss:0.030, train acc:0.9940, test acc:0.6140\n",
      "[365,300] loss:0.032, train acc:0.9950, test acc:0.6180\n",
      "[366,300] loss:0.029, train acc:0.9940, test acc:0.6200\n",
      "[367,300] loss:0.029, train acc:0.9920, test acc:0.6210\n",
      "[368,300] loss:0.028, train acc:0.9940, test acc:0.6230\n",
      "[369,300] loss:0.028, train acc:0.9920, test acc:0.6200\n",
      "[370,300] loss:0.028, train acc:0.9960, test acc:0.6170\n",
      "[371,300] loss:0.028, train acc:0.9950, test acc:0.6230\n",
      "[372,300] loss:0.030, train acc:0.9940, test acc:0.6230\n",
      "[373,300] loss:0.030, train acc:0.9950, test acc:0.6190\n",
      "[374,300] loss:0.030, train acc:0.9950, test acc:0.6300\n",
      "[375,300] loss:0.029, train acc:0.9920, test acc:0.6370\n",
      "[376,300] loss:0.030, train acc:0.9930, test acc:0.6140\n",
      "[377,300] loss:0.029, train acc:0.9940, test acc:0.6150\n",
      "[378,300] loss:0.027, train acc:0.9940, test acc:0.6360\n",
      "[379,300] loss:0.027, train acc:0.9940, test acc:0.6360\n",
      "[380,300] loss:0.027, train acc:0.9950, test acc:0.6340\n",
      "[381,300] loss:0.027, train acc:0.9950, test acc:0.6280\n",
      "[382,300] loss:0.028, train acc:0.9930, test acc:0.6060\n",
      "[383,300] loss:0.027, train acc:0.9930, test acc:0.6250\n",
      "[384,300] loss:0.027, train acc:0.9940, test acc:0.6320\n",
      "[385,300] loss:0.026, train acc:0.9940, test acc:0.6250\n",
      "[386,300] loss:0.029, train acc:0.9930, test acc:0.6140\n",
      "[387,300] loss:0.027, train acc:0.9940, test acc:0.6300\n",
      "[388,300] loss:0.027, train acc:0.9930, test acc:0.6080\n",
      "[389,300] loss:0.026, train acc:0.9950, test acc:0.6140\n",
      "[390,300] loss:0.026, train acc:0.9940, test acc:0.6180\n",
      "[391,300] loss:0.028, train acc:0.9950, test acc:0.6090\n",
      "[392,300] loss:0.026, train acc:0.9950, test acc:0.6110\n",
      "[393,300] loss:0.026, train acc:0.9930, test acc:0.6170\n",
      "[394,300] loss:0.026, train acc:0.9940, test acc:0.6240\n",
      "[395,300] loss:0.025, train acc:0.9950, test acc:0.6190\n",
      "[396,300] loss:0.025, train acc:0.9950, test acc:0.6260\n",
      "[397,300] loss:0.025, train acc:0.9930, test acc:0.6060\n",
      "[398,300] loss:0.025, train acc:0.9950, test acc:0.6150\n",
      "[399,300] loss:0.025, train acc:0.9930, test acc:0.6180\n",
      "[400,300] loss:0.024, train acc:0.9950, test acc:0.6160\n",
      "[401,300] loss:0.024, train acc:0.9930, test acc:0.6120\n",
      "[402,300] loss:0.025, train acc:0.9930, test acc:0.6180\n",
      "[403,300] loss:0.030, train acc:0.9910, test acc:0.6240\n",
      "[404,300] loss:0.032, train acc:0.9950, test acc:0.5950\n",
      "[405,300] loss:0.036, train acc:0.9950, test acc:0.6290\n",
      "[406,300] loss:0.031, train acc:0.9950, test acc:0.6200\n",
      "[407,300] loss:0.030, train acc:0.9930, test acc:0.6270\n",
      "[408,300] loss:0.042, train acc:0.9940, test acc:0.6170\n",
      "[409,300] loss:0.052, train acc:0.9890, test acc:0.6330\n",
      "[410,300] loss:0.116, train acc:0.9850, test acc:0.5810\n",
      "[411,300] loss:0.097, train acc:0.9800, test acc:0.5890\n",
      "[412,300] loss:0.066, train acc:0.9880, test acc:0.6220\n",
      "[413,300] loss:0.061, train acc:0.9900, test acc:0.6400\n",
      "[414,300] loss:0.060, train acc:0.9900, test acc:0.6320\n",
      "[415,300] loss:0.073, train acc:0.9840, test acc:0.6370\n",
      "[416,300] loss:0.101, train acc:0.9730, test acc:0.6290\n",
      "[417,300] loss:0.090, train acc:0.9850, test acc:0.6300\n",
      "[418,300] loss:0.075, train acc:0.9870, test acc:0.6140\n",
      "[419,300] loss:0.052, train acc:0.9900, test acc:0.6420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1700068/2755609872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [batch, config.frames, config.N_hid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1700068/3417643451.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         '''\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# out shape [batch, channel, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out shape [batch, channel, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# out shape [batch, channel, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# out shape [batch, channel]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0m\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = train_rs[:,1:,0:config.N_hid]\n",
    "y_train = train_label\n",
    "X_test = test_rs[:,1:,0:config.N_hid]\n",
    "y_test = test_label\n",
    "\n",
    "train_num = X_train.shape[0]\n",
    "test_num = X_test.shape[0]\n",
    "iteration = int(train_num/config.batch_size)\n",
    "iter = int(test_num/config.batch_size)\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=configuration.lr)\n",
    "for epoch in range(configuration.epoch) :\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    train_correct = 0\n",
    "    for i in range(iteration):\n",
    "        x = X_train[i*config.batch_size:(i+1)*config.batch_size] # [batch, config.frames, config.N_hid]\n",
    "        y = y_train[i*config.batch_size:(i+1)*config.batch_size]\n",
    "        out = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = cost(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, id = torch.max(out.data, 1)\n",
    "        sum_loss += loss.data\n",
    "        train_correct+=torch.sum(id==y.data)\n",
    "    \n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    for i in range(iter):\n",
    "        x = X_test[i*config.batch_size:(i+1)*config.batch_size]\n",
    "        y = y_test[i*config.batch_size:(i+1)*config.batch_size]\n",
    "        outputs = model(x)\n",
    "        _, id = torch.max(outputs.data, 1)\n",
    "        test_correct += torch.sum(id == y.data)\n",
    "    if config.verbose:\n",
    "        print('[%d,%d] loss:%.03f, train acc:%.4f, test acc:%.4f' % (epoch+1, configuration.epoch, sum_loss/iteration, train_correct/train_num, test_correct/test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 30, 1000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1700068/2594976888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_transformer_readout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_hid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_hid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1700068/1681433792.py\u001b[0m in \u001b[0;36mtrain_transformer_readout\u001b[0;34m(model, config, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "train_transformer_readout(model,configuration, train_rs[:,1:,0:config.N_hid], train_label, test_rs[:,1:,0:config.N_hid], test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in avgpool.named_parameters():\n",
    "    print(i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generation Finish\n",
      "finish inference\n",
      "finish inference\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import inference_new\n",
    "run_time = time.strftime(\"%Y.%m.%d-%H-%M-%S\", time.localtime())\n",
    "# param_search(run_time)\n",
    "from config import Config\n",
    "from data import part_DATA\n",
    "from RC import torchRC, EGAT, EGCN\n",
    "import dgl\n",
    "from RC import MLP\n",
    "\n",
    "config = Config()\n",
    "config.device = 'cpu'\n",
    "config.data = 'mnist'\n",
    "config.train_num = 1000\n",
    "config.test_num = 1000\n",
    "config.N_in = 28*28\n",
    "config.N_out = 10\n",
    "train_loader, test_loader = part_DATA(config)\n",
    "\n",
    "model = torchRC(config).to(config.device)\n",
    "train_rs, train_label = inference_new(model, config, train_loader,)\n",
    "test_rs, test_label = inference_new(model, config, test_loader,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 31, 2000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,300] loss:1.843, train acc:0.4879, test acc:0.6850\n",
      "[2,300] loss:1.175, train acc:0.7510, test acc:0.7850\n",
      "[3,300] loss:0.856, train acc:0.8098, test acc:0.8200\n",
      "[4,300] loss:0.688, train acc:0.8372, test acc:0.8465\n",
      "[5,300] loss:0.593, train acc:0.8537, test acc:0.8595\n",
      "[6,300] loss:0.532, train acc:0.8639, test acc:0.8715\n",
      "[7,300] loss:0.489, train acc:0.8716, test acc:0.8770\n",
      "[8,300] loss:0.456, train acc:0.8772, test acc:0.8835\n",
      "[9,300] loss:0.430, train acc:0.8825, test acc:0.8865\n",
      "[10,300] loss:0.408, train acc:0.8875, test acc:0.8920\n",
      "[11,300] loss:0.390, train acc:0.8912, test acc:0.8930\n",
      "[12,300] loss:0.375, train acc:0.8948, test acc:0.8945\n",
      "[13,300] loss:0.360, train acc:0.8995, test acc:0.8970\n",
      "[14,300] loss:0.348, train acc:0.9036, test acc:0.8990\n",
      "[15,300] loss:0.336, train acc:0.9064, test acc:0.9010\n",
      "[16,300] loss:0.327, train acc:0.9096, test acc:0.9030\n",
      "[17,300] loss:0.318, train acc:0.9125, test acc:0.9065\n",
      "[18,300] loss:0.309, train acc:0.9152, test acc:0.9070\n",
      "[19,300] loss:0.302, train acc:0.9169, test acc:0.9075\n",
      "[20,300] loss:0.295, train acc:0.9189, test acc:0.9080\n",
      "[21,300] loss:0.288, train acc:0.9215, test acc:0.9085\n",
      "[22,300] loss:0.282, train acc:0.9229, test acc:0.9095\n",
      "[23,300] loss:0.276, train acc:0.9248, test acc:0.9095\n",
      "[24,300] loss:0.270, train acc:0.9264, test acc:0.9100\n",
      "[25,300] loss:0.265, train acc:0.9280, test acc:0.9105\n",
      "[26,300] loss:0.260, train acc:0.9294, test acc:0.9105\n",
      "[27,300] loss:0.255, train acc:0.9308, test acc:0.9120\n",
      "[28,300] loss:0.250, train acc:0.9320, test acc:0.9120\n",
      "[29,300] loss:0.246, train acc:0.9328, test acc:0.9130\n",
      "[30,300] loss:0.242, train acc:0.9335, test acc:0.9145\n",
      "[31,300] loss:0.238, train acc:0.9342, test acc:0.9160\n",
      "[32,300] loss:0.234, train acc:0.9356, test acc:0.9165\n",
      "[33,300] loss:0.230, train acc:0.9369, test acc:0.9180\n",
      "[34,300] loss:0.226, train acc:0.9377, test acc:0.9195\n",
      "[35,300] loss:0.223, train acc:0.9383, test acc:0.9195\n",
      "[36,300] loss:0.219, train acc:0.9390, test acc:0.9195\n",
      "[37,300] loss:0.216, train acc:0.9402, test acc:0.9215\n",
      "[38,300] loss:0.213, train acc:0.9415, test acc:0.9230\n",
      "[39,300] loss:0.209, train acc:0.9420, test acc:0.9230\n",
      "[40,300] loss:0.206, train acc:0.9424, test acc:0.9235\n",
      "[41,300] loss:0.203, train acc:0.9431, test acc:0.9235\n",
      "[42,300] loss:0.200, train acc:0.9440, test acc:0.9230\n",
      "[43,300] loss:0.197, train acc:0.9454, test acc:0.9230\n",
      "[44,300] loss:0.195, train acc:0.9454, test acc:0.9245\n",
      "[45,300] loss:0.192, train acc:0.9465, test acc:0.9250\n",
      "[46,300] loss:0.189, train acc:0.9470, test acc:0.9250\n",
      "[47,300] loss:0.187, train acc:0.9476, test acc:0.9250\n",
      "[48,300] loss:0.184, train acc:0.9485, test acc:0.9260\n",
      "[49,300] loss:0.182, train acc:0.9497, test acc:0.9265\n",
      "[50,300] loss:0.179, train acc:0.9506, test acc:0.9270\n",
      "[51,300] loss:0.177, train acc:0.9514, test acc:0.9275\n",
      "[52,300] loss:0.174, train acc:0.9522, test acc:0.9270\n",
      "[53,300] loss:0.172, train acc:0.9530, test acc:0.9270\n",
      "[54,300] loss:0.170, train acc:0.9535, test acc:0.9265\n",
      "[55,300] loss:0.168, train acc:0.9539, test acc:0.9265\n",
      "[56,300] loss:0.165, train acc:0.9543, test acc:0.9275\n",
      "[57,300] loss:0.163, train acc:0.9548, test acc:0.9275\n",
      "[58,300] loss:0.161, train acc:0.9556, test acc:0.9275\n",
      "[59,300] loss:0.159, train acc:0.9561, test acc:0.9275\n",
      "[60,300] loss:0.157, train acc:0.9566, test acc:0.9275\n",
      "[61,300] loss:0.155, train acc:0.9570, test acc:0.9280\n",
      "[62,300] loss:0.153, train acc:0.9581, test acc:0.9280\n",
      "[63,300] loss:0.151, train acc:0.9590, test acc:0.9290\n",
      "[64,300] loss:0.149, train acc:0.9593, test acc:0.9290\n",
      "[65,300] loss:0.147, train acc:0.9602, test acc:0.9290\n",
      "[66,300] loss:0.146, train acc:0.9610, test acc:0.9290\n",
      "[67,300] loss:0.144, train acc:0.9612, test acc:0.9295\n",
      "[68,300] loss:0.142, train acc:0.9621, test acc:0.9295\n",
      "[69,300] loss:0.140, train acc:0.9628, test acc:0.9300\n",
      "[70,300] loss:0.139, train acc:0.9635, test acc:0.9305\n",
      "[71,300] loss:0.137, train acc:0.9638, test acc:0.9315\n",
      "[72,300] loss:0.135, train acc:0.9642, test acc:0.9320\n",
      "[73,300] loss:0.134, train acc:0.9645, test acc:0.9325\n",
      "[74,300] loss:0.132, train acc:0.9651, test acc:0.9330\n",
      "[75,300] loss:0.130, train acc:0.9657, test acc:0.9335\n",
      "[76,300] loss:0.129, train acc:0.9666, test acc:0.9345\n",
      "[77,300] loss:0.127, train acc:0.9674, test acc:0.9340\n",
      "[78,300] loss:0.126, train acc:0.9676, test acc:0.9340\n",
      "[79,300] loss:0.124, train acc:0.9683, test acc:0.9340\n",
      "[80,300] loss:0.123, train acc:0.9696, test acc:0.9345\n",
      "[81,300] loss:0.121, train acc:0.9699, test acc:0.9345\n",
      "[82,300] loss:0.120, train acc:0.9705, test acc:0.9355\n",
      "[83,300] loss:0.118, train acc:0.9716, test acc:0.9350\n",
      "[84,300] loss:0.117, train acc:0.9723, test acc:0.9360\n",
      "[85,300] loss:0.115, train acc:0.9732, test acc:0.9365\n",
      "[86,300] loss:0.114, train acc:0.9736, test acc:0.9365\n",
      "[87,300] loss:0.113, train acc:0.9739, test acc:0.9370\n",
      "[88,300] loss:0.111, train acc:0.9742, test acc:0.9365\n",
      "[89,300] loss:0.110, train acc:0.9752, test acc:0.9370\n",
      "[90,300] loss:0.109, train acc:0.9760, test acc:0.9370\n",
      "[91,300] loss:0.107, train acc:0.9765, test acc:0.9365\n",
      "[92,300] loss:0.106, train acc:0.9769, test acc:0.9365\n",
      "[93,300] loss:0.105, train acc:0.9771, test acc:0.9360\n",
      "[94,300] loss:0.104, train acc:0.9773, test acc:0.9360\n",
      "[95,300] loss:0.102, train acc:0.9777, test acc:0.9360\n",
      "[96,300] loss:0.101, train acc:0.9781, test acc:0.9365\n",
      "[97,300] loss:0.100, train acc:0.9783, test acc:0.9355\n",
      "[98,300] loss:0.099, train acc:0.9785, test acc:0.9360\n",
      "[99,300] loss:0.098, train acc:0.9789, test acc:0.9360\n",
      "[100,300] loss:0.096, train acc:0.9800, test acc:0.9360\n",
      "[101,300] loss:0.095, train acc:0.9803, test acc:0.9360\n",
      "[102,300] loss:0.094, train acc:0.9807, test acc:0.9360\n",
      "[103,300] loss:0.093, train acc:0.9811, test acc:0.9360\n",
      "[104,300] loss:0.092, train acc:0.9816, test acc:0.9360\n",
      "[105,300] loss:0.091, train acc:0.9818, test acc:0.9360\n",
      "[106,300] loss:0.090, train acc:0.9822, test acc:0.9355\n",
      "[107,300] loss:0.088, train acc:0.9825, test acc:0.9355\n",
      "[108,300] loss:0.087, train acc:0.9827, test acc:0.9360\n",
      "[109,300] loss:0.086, train acc:0.9834, test acc:0.9360\n",
      "[110,300] loss:0.085, train acc:0.9838, test acc:0.9365\n",
      "[111,300] loss:0.084, train acc:0.9846, test acc:0.9365\n",
      "[112,300] loss:0.083, train acc:0.9852, test acc:0.9365\n",
      "[113,300] loss:0.082, train acc:0.9853, test acc:0.9365\n",
      "[114,300] loss:0.081, train acc:0.9854, test acc:0.9360\n",
      "[115,300] loss:0.080, train acc:0.9855, test acc:0.9355\n",
      "[116,300] loss:0.079, train acc:0.9859, test acc:0.9355\n",
      "[117,300] loss:0.078, train acc:0.9861, test acc:0.9355\n",
      "[118,300] loss:0.077, train acc:0.9863, test acc:0.9355\n",
      "[119,300] loss:0.076, train acc:0.9866, test acc:0.9355\n",
      "[120,300] loss:0.075, train acc:0.9866, test acc:0.9350\n",
      "[121,300] loss:0.075, train acc:0.9869, test acc:0.9350\n",
      "[122,300] loss:0.074, train acc:0.9870, test acc:0.9345\n",
      "[123,300] loss:0.073, train acc:0.9871, test acc:0.9345\n",
      "[124,300] loss:0.072, train acc:0.9874, test acc:0.9345\n",
      "[125,300] loss:0.071, train acc:0.9878, test acc:0.9345\n",
      "[126,300] loss:0.070, train acc:0.9881, test acc:0.9350\n",
      "[127,300] loss:0.069, train acc:0.9883, test acc:0.9350\n",
      "[128,300] loss:0.068, train acc:0.9885, test acc:0.9355\n",
      "[129,300] loss:0.068, train acc:0.9887, test acc:0.9355\n",
      "[130,300] loss:0.067, train acc:0.9893, test acc:0.9355\n",
      "[131,300] loss:0.066, train acc:0.9897, test acc:0.9360\n",
      "[132,300] loss:0.065, train acc:0.9898, test acc:0.9365\n",
      "[133,300] loss:0.064, train acc:0.9900, test acc:0.9360\n",
      "[134,300] loss:0.063, train acc:0.9899, test acc:0.9360\n",
      "[135,300] loss:0.063, train acc:0.9901, test acc:0.9360\n",
      "[136,300] loss:0.062, train acc:0.9904, test acc:0.9360\n",
      "[137,300] loss:0.061, train acc:0.9904, test acc:0.9365\n",
      "[138,300] loss:0.060, train acc:0.9906, test acc:0.9360\n",
      "[139,300] loss:0.060, train acc:0.9907, test acc:0.9360\n",
      "[140,300] loss:0.059, train acc:0.9910, test acc:0.9365\n",
      "[141,300] loss:0.058, train acc:0.9912, test acc:0.9365\n",
      "[142,300] loss:0.057, train acc:0.9913, test acc:0.9360\n",
      "[143,300] loss:0.057, train acc:0.9916, test acc:0.9360\n",
      "[144,300] loss:0.056, train acc:0.9919, test acc:0.9360\n",
      "[145,300] loss:0.055, train acc:0.9923, test acc:0.9365\n",
      "[146,300] loss:0.054, train acc:0.9924, test acc:0.9365\n",
      "[147,300] loss:0.054, train acc:0.9928, test acc:0.9365\n",
      "[148,300] loss:0.053, train acc:0.9931, test acc:0.9365\n",
      "[149,300] loss:0.052, train acc:0.9931, test acc:0.9365\n",
      "[150,300] loss:0.052, train acc:0.9932, test acc:0.9365\n",
      "[151,300] loss:0.051, train acc:0.9935, test acc:0.9365\n",
      "[152,300] loss:0.050, train acc:0.9936, test acc:0.9365\n",
      "[153,300] loss:0.050, train acc:0.9938, test acc:0.9365\n",
      "[154,300] loss:0.049, train acc:0.9939, test acc:0.9365\n",
      "[155,300] loss:0.048, train acc:0.9943, test acc:0.9365\n",
      "[156,300] loss:0.048, train acc:0.9945, test acc:0.9365\n",
      "[157,300] loss:0.047, train acc:0.9949, test acc:0.9365\n",
      "[158,300] loss:0.046, train acc:0.9951, test acc:0.9360\n",
      "[159,300] loss:0.046, train acc:0.9952, test acc:0.9360\n",
      "[160,300] loss:0.045, train acc:0.9952, test acc:0.9360\n",
      "[161,300] loss:0.044, train acc:0.9953, test acc:0.9370\n",
      "[162,300] loss:0.044, train acc:0.9954, test acc:0.9365\n",
      "[163,300] loss:0.043, train acc:0.9954, test acc:0.9360\n",
      "[164,300] loss:0.043, train acc:0.9954, test acc:0.9365\n",
      "[165,300] loss:0.042, train acc:0.9955, test acc:0.9360\n",
      "[166,300] loss:0.042, train acc:0.9958, test acc:0.9360\n",
      "[167,300] loss:0.041, train acc:0.9959, test acc:0.9365\n",
      "[168,300] loss:0.040, train acc:0.9961, test acc:0.9360\n",
      "[169,300] loss:0.040, train acc:0.9962, test acc:0.9360\n",
      "[170,300] loss:0.039, train acc:0.9962, test acc:0.9360\n",
      "[171,300] loss:0.039, train acc:0.9963, test acc:0.9360\n",
      "[172,300] loss:0.038, train acc:0.9965, test acc:0.9365\n",
      "[173,300] loss:0.038, train acc:0.9967, test acc:0.9360\n",
      "[174,300] loss:0.037, train acc:0.9967, test acc:0.9360\n",
      "[175,300] loss:0.037, train acc:0.9968, test acc:0.9360\n",
      "[176,300] loss:0.036, train acc:0.9972, test acc:0.9360\n",
      "[177,300] loss:0.036, train acc:0.9972, test acc:0.9365\n",
      "[178,300] loss:0.035, train acc:0.9974, test acc:0.9360\n",
      "[179,300] loss:0.035, train acc:0.9974, test acc:0.9360\n",
      "[180,300] loss:0.034, train acc:0.9975, test acc:0.9365\n",
      "[181,300] loss:0.034, train acc:0.9976, test acc:0.9360\n",
      "[182,300] loss:0.033, train acc:0.9980, test acc:0.9365\n",
      "[183,300] loss:0.033, train acc:0.9982, test acc:0.9370\n",
      "[184,300] loss:0.032, train acc:0.9983, test acc:0.9365\n",
      "[185,300] loss:0.032, train acc:0.9983, test acc:0.9370\n",
      "[186,300] loss:0.031, train acc:0.9983, test acc:0.9365\n",
      "[187,300] loss:0.031, train acc:0.9983, test acc:0.9370\n",
      "[188,300] loss:0.030, train acc:0.9985, test acc:0.9370\n",
      "[189,300] loss:0.030, train acc:0.9985, test acc:0.9370\n",
      "[190,300] loss:0.029, train acc:0.9985, test acc:0.9375\n",
      "[191,300] loss:0.029, train acc:0.9985, test acc:0.9380\n",
      "[192,300] loss:0.028, train acc:0.9985, test acc:0.9375\n",
      "[193,300] loss:0.028, train acc:0.9986, test acc:0.9370\n",
      "[194,300] loss:0.028, train acc:0.9986, test acc:0.9380\n",
      "[195,300] loss:0.027, train acc:0.9986, test acc:0.9375\n",
      "[196,300] loss:0.027, train acc:0.9986, test acc:0.9375\n",
      "[197,300] loss:0.026, train acc:0.9987, test acc:0.9375\n",
      "[198,300] loss:0.026, train acc:0.9987, test acc:0.9375\n",
      "[199,300] loss:0.026, train acc:0.9987, test acc:0.9385\n",
      "[200,300] loss:0.025, train acc:0.9988, test acc:0.9380\n",
      "[201,300] loss:0.025, train acc:0.9988, test acc:0.9380\n",
      "[202,300] loss:0.024, train acc:0.9988, test acc:0.9380\n",
      "[203,300] loss:0.024, train acc:0.9988, test acc:0.9385\n",
      "[204,300] loss:0.024, train acc:0.9989, test acc:0.9375\n",
      "[205,300] loss:0.023, train acc:0.9990, test acc:0.9380\n",
      "[206,300] loss:0.023, train acc:0.9990, test acc:0.9380\n",
      "[207,300] loss:0.023, train acc:0.9990, test acc:0.9375\n",
      "[208,300] loss:0.022, train acc:0.9990, test acc:0.9385\n",
      "[209,300] loss:0.022, train acc:0.9990, test acc:0.9375\n",
      "[210,300] loss:0.021, train acc:0.9991, test acc:0.9375\n",
      "[211,300] loss:0.021, train acc:0.9991, test acc:0.9375\n",
      "[212,300] loss:0.021, train acc:0.9991, test acc:0.9380\n",
      "[213,300] loss:0.020, train acc:0.9991, test acc:0.9375\n",
      "[214,300] loss:0.020, train acc:0.9991, test acc:0.9375\n",
      "[215,300] loss:0.020, train acc:0.9991, test acc:0.9380\n",
      "[216,300] loss:0.019, train acc:0.9991, test acc:0.9375\n",
      "[217,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[218,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[219,300] loss:0.019, train acc:0.9992, test acc:0.9375\n",
      "[220,300] loss:0.018, train acc:0.9992, test acc:0.9375\n",
      "[221,300] loss:0.018, train acc:0.9993, test acc:0.9375\n",
      "[222,300] loss:0.018, train acc:0.9993, test acc:0.9375\n",
      "[223,300] loss:0.017, train acc:0.9994, test acc:0.9375\n",
      "[224,300] loss:0.017, train acc:0.9995, test acc:0.9375\n",
      "[225,300] loss:0.017, train acc:0.9995, test acc:0.9380\n",
      "[226,300] loss:0.016, train acc:0.9995, test acc:0.9375\n",
      "[227,300] loss:0.016, train acc:0.9995, test acc:0.9375\n",
      "[228,300] loss:0.016, train acc:0.9996, test acc:0.9375\n",
      "[229,300] loss:0.016, train acc:0.9996, test acc:0.9375\n",
      "[230,300] loss:0.015, train acc:0.9996, test acc:0.9375\n",
      "[231,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[232,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[233,300] loss:0.015, train acc:0.9997, test acc:0.9375\n",
      "[234,300] loss:0.014, train acc:0.9997, test acc:0.9380\n",
      "[235,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[236,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[237,300] loss:0.014, train acc:0.9997, test acc:0.9375\n",
      "[238,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[239,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[240,300] loss:0.013, train acc:0.9997, test acc:0.9375\n",
      "[241,300] loss:0.013, train acc:0.9998, test acc:0.9370\n",
      "[242,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[243,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[244,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[245,300] loss:0.012, train acc:0.9998, test acc:0.9375\n",
      "[246,300] loss:0.012, train acc:0.9999, test acc:0.9375\n",
      "[247,300] loss:0.011, train acc:0.9999, test acc:0.9370\n",
      "[248,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[249,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[250,300] loss:0.011, train acc:0.9999, test acc:0.9375\n",
      "[251,300] loss:0.011, train acc:1.0000, test acc:0.9370\n",
      "[252,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[253,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[254,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[255,300] loss:0.010, train acc:1.0000, test acc:0.9375\n",
      "[256,300] loss:0.010, train acc:1.0000, test acc:0.9380\n",
      "[257,300] loss:0.010, train acc:1.0000, test acc:0.9380\n",
      "[258,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[259,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[260,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[261,300] loss:0.009, train acc:1.0000, test acc:0.9380\n",
      "[262,300] loss:0.009, train acc:1.0000, test acc:0.9375\n",
      "[263,300] loss:0.009, train acc:1.0000, test acc:0.9370\n",
      "[264,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[265,300] loss:0.008, train acc:1.0000, test acc:0.9365\n",
      "[266,300] loss:0.008, train acc:1.0000, test acc:0.9375\n",
      "[267,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[268,300] loss:0.008, train acc:1.0000, test acc:0.9365\n",
      "[269,300] loss:0.008, train acc:1.0000, test acc:0.9370\n",
      "[270,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[271,300] loss:0.007, train acc:1.0000, test acc:0.9355\n",
      "[272,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[273,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[274,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[275,300] loss:0.007, train acc:1.0000, test acc:0.9360\n",
      "[276,300] loss:0.007, train acc:1.0000, test acc:0.9370\n",
      "[277,300] loss:0.007, train acc:1.0000, test acc:0.9365\n",
      "[278,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[279,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[280,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[281,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[282,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[283,300] loss:0.006, train acc:1.0000, test acc:0.9365\n",
      "[284,300] loss:0.006, train acc:1.0000, test acc:0.9365\n",
      "[285,300] loss:0.006, train acc:1.0000, test acc:0.9350\n",
      "[286,300] loss:0.006, train acc:1.0000, test acc:0.9360\n",
      "[287,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[288,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[289,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[290,300] loss:0.005, train acc:1.0000, test acc:0.9350\n",
      "[291,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[292,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[293,300] loss:0.005, train acc:1.0000, test acc:0.9350\n",
      "[294,300] loss:0.005, train acc:1.0000, test acc:0.9360\n",
      "[295,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[296,300] loss:0.005, train acc:1.0000, test acc:0.9355\n",
      "[297,300] loss:0.004, train acc:1.0000, test acc:0.9355\n",
      "[298,300] loss:0.004, train acc:1.0000, test acc:0.9345\n",
      "[299,300] loss:0.004, train acc:1.0000, test acc:0.9350\n",
      "[300,300] loss:0.004, train acc:1.0000, test acc:0.9355\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import train_mlp_readout\n",
    "config.epoch = 300\n",
    "\n",
    "# reduce frame dimension\n",
    "train_rs = train_rs[:,1:,:].mean(1)\n",
    "test_rs = test_rs[:,1:,:].mean(1)\n",
    "\n",
    "mlp = MLP(2*config.N_hid, config.mlp_hid, config.N_out).to(model.device)\n",
    "train_score, test_score, = train_mlp_readout(model=mlp, \n",
    "                                            config=config,\n",
    "                                            X_train=train_rs,\n",
    "                                            X_test=test_rs,\n",
    "                                            y_train=train_label,\n",
    "                                            y_test=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = train_rs[index][:,1:,0:config.N_hid]\n",
    "# v = v.transpose(1,2).view(-1,30)\n",
    "# node_feat = Egcn(g, v[0:config.N_hid], edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 4, 7, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[index].view(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 200])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v = v.transpose(1,2).reshape(-1, 30)\n",
    "batch_g = dgl.batch([g]*5)\n",
    "batch_edge_attr = torch.cat(([edge_attr]*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = Egcn(batch_g, batch_v, batch_edge_attr)\n",
    "# pred = node_feat.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\44670\\Documents\\GitHub\\Reservoir-Computing\\learn_dgl.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_label' is not defined"
     ]
    }
   ],
   "source": [
    "batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 5, 5, 9, 9, 9, 8, 8, 9])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1536, 512])\n",
      "torch.Size([1536])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i in transformer_encoder.named_parameters():\n",
    "    print(i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68160"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*30+64+64*32*32+310+330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\snn\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:447: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rst = self._activation(rst)\n"
     ]
    }
   ],
   "source": [
    "u = [0,1,2,3,2,5]\n",
    "v = [1,2,3,4,0,3]\n",
    "g = dgl.graph((u,v))\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "batch_g = dgl.batch([g, g])\n",
    "feat = torch.ones(6, 3)\n",
    "batch_f = torch.cat([feat, feat])\n",
    "\n",
    "edge_weight = torch.tensor([0.5, 0.6, 0.4, 0.7, 0.9, 0.1, 1, 1, 1, 1, 1, 1])\n",
    "norm = EdgeWeightNorm(norm='both')\n",
    "norm_edge_weight = norm(g, edge_weight)\n",
    "conv1 = GraphConv(3, 2, norm='none', weight=True, bias=True, activation=None)\n",
    "conv2 = GraphConv(3, 2, norm='none', weight=True, bias=True, activation=nn.Softmax())\n",
    "conv2.weight = conv1.weight\n",
    "conv2.bias = conv1.bias\n",
    "\n",
    "# out1 = conv(batch_g, batch_f, edge_weight=norm_edge_weight)\n",
    "out2 = conv1(batch_g, batch_f, edge_weight=None)\n",
    "out3 = conv2(batch_g, batch_f, edge_weight=None)\n",
    "# out3 = conv(g, feat, edge_weight=edge_weight)\n",
    "# print(out1,'\\n', out2, '\\n', out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-4.7213,  1.2245],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-1.5738,  0.4082],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-4.7213,  1.2245],\n",
       "         [-3.1475,  0.8163],\n",
       "         [-1.5738,  0.4082]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0026, 0.9974],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.1211, 0.8789],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.0026, 0.9974],\n",
       "         [0.0186, 0.9814],\n",
       "         [0.1211, 0.8789]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2,out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\GitHubClone\\Reservoir-Computing\\learn_dgl.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/GitHubClone/Reservoir-Computing/learn_dgl.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39madjacency_matrix()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/GitHubClone/Reservoir-Computing/learn_dgl.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert Sparse layout tensor to numpy.convert the tensor to a strided layout first."
     ]
    }
   ],
   "source": [
    "a = graph.adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 15]), torch.Size([30, 3, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes, num_edges = 8, 30\n",
    "node_dim = 20\n",
    "edge_dim = 1\n",
    "graph = dgl.rand_graph(num_nodes, num_edges)\n",
    "node_feats = torch.rand((num_nodes, node_dim))\n",
    "edge_feats = torch.rand((num_edges, edge_dim))\n",
    "egat = EGATConv(in_node_feats=node_dim,\n",
    "                in_edge_feats=edge_dim,\n",
    "                out_node_feats=15,\n",
    "                out_edge_feats=1,\n",
    "                num_heads=3)\n",
    "#forward pass\n",
    "new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "new_node_feats.shape, new_edge_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7398],\n",
       "        [2.1219],\n",
       "        [0.2601]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edge_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3739], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edge_feats.mean(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Number of categories: 7\n",
      "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset('./data')\n",
    "print('Number of categories:', dataset.num_classes)\n",
    "g = dataset[0]\n",
    "print(g.ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\44670\\Documents\\GitHub\\Reservoir-Computing\\learn_dgl.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mif\u001b[39;00m e \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIn epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, val acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m (best \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m), test acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m (best \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                 e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m model \u001b[39m=\u001b[39m GCN(g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m16\u001b[39m, dataset\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/44670/Documents/GitHub/Reservoir-Computing/learn_dgl.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m train(g, model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GCN' is not defined"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(30):\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(1)\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 101, 200])\n"
     ]
    }
   ],
   "source": [
    "from config import Config as config\n",
    "from RC import torchRC\n",
    "model = torchRC(config)\n",
    "mem, spike = model(torch.rand(16, config.frames, 50))\n",
    "print(mem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.reservoir.A.numpy()\n",
    "edge_index = torch.tensor(np.where(A!=0), dtype=torch.long)\n",
    "edge_attr = torch.tensor(np.array([A[i,j] for i,j in edge_index.T]))\n",
    "u = edge_index[0]\n",
    "v = edge_index[1]\n",
    "mems = mem[0,0,1:].T\n",
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = mems\n",
    "g.edata['w'] = edge_attr\n",
    "\n",
    "h = dgl.graph((u, v))\n",
    "h.ndata['x'] = mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = mems\n",
    "g.edata['w'] = edge_attr\n",
    "\n",
    "h = dgl.graph((u, v))\n",
    "h.ndata['x'] = mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "conv1 = GraphConv(config.frames, 16)\n",
    "out = conv1(g, mems)\n",
    "out_h = conv1(h, mems)\n",
    "print(out_h[0])\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3371, -0.0486,  0.0422,  ...,  0.0489, -0.0459,  0.3718])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "conv2 = GCNConv(100, 16)\n",
    "conv2(mems, edge_index, edge_attr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71755e07d76f363f96e0c11ca35acb8f13d731f42c5e22d3b26f54f837a1c4d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
