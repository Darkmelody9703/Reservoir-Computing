{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from utils import A_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    input = 700\n",
    "    output = 20\n",
    "    hid = 128    # RC Neurons\n",
    "    thr = 0.5\n",
    "    decay = 0.5\n",
    "    rst = 0.05\n",
    "    \n",
    "    N_hid = hid\n",
    "    p_in = 0.2        # ratio of inhibitory neurons\n",
    "    gamma = 1.0       # shape factor of gamma distribution\n",
    "    binary = False    # binary matrix of reservoir A\n",
    "    net_type = 'BAC'  # type of reservoir connection topology\n",
    "                      # 'ER',  # Erdos-Renyi Random Network\n",
    "                      # 'ERC', # Clusters of Erdos-Renyi Networks\n",
    "                      # 'BA',  # Barabasi-Albert Network\n",
    "                      # 'BAC', # Clusters of Barabasi-Albert networks\n",
    "                      # 'WS',  # Watts Strogatz small world networks\n",
    "                      # 'WSC', # Clusters of Watts Strogatz small world networks\n",
    "                      # 'RAN', # random network\n",
    "                      # 'DTW', # Developmental Time Window for multi-cluster small-world network\n",
    "    noise = True      # add noise in A\n",
    "    noise_str = 0.05  # noise strength\n",
    "    p_ER = 0.2        # connection probability when creating edges, for ER and WS graphs\n",
    "    m_BA = 3          # number of edges to attach from a new node to existing nodes\n",
    "    k = 5             # number of clusters in A\n",
    "    R = 0.2           # distance factor when deciding connections in random network\n",
    "    scale = False     # rescale matrix A with spectral radius\n",
    "    \n",
    "    \n",
    "    batch = 32\n",
    "    epoch = 50\n",
    "    lr = 0.01\n",
    "    device = torch.device('cuda:0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\extract] manually, then SpikingJelly will re-extract files from [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\download].\n",
      "The directory [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\frames_number_20_split_by_number] already exists.\n",
      "The directory [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\extract] manually, then SpikingJelly will re-extract files from [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\download].\n",
      "The directory [D:\\Ph.D\\Research\\SNN-SRT数据\\SHD\\frames_number_20_split_by_number] already exists.\n"
     ]
    }
   ],
   "source": [
    "from spikingjelly.datasets.shd import SpikingHeidelbergDigits\n",
    "\n",
    "SHD_train = SpikingHeidelbergDigits('D:\\Ph.D\\Research\\SNN-SRT数据\\SHD', train=True, data_type='frame', frames_number=20, split_by='number')\n",
    "SHD_test = SpikingHeidelbergDigits('D:\\Ph.D\\Research\\SNN-SRT数据\\SHD', train=False, data_type='frame', frames_number=20, split_by='number')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=SHD_train, batch_size=config.batch, shuffle=True, drop_last=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=SHD_test, batch_size=config.batch, shuffle=False, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActFun(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        temp = abs(input - 0) < 0.5 # lens\n",
    "        return grad_input * temp.float()\n",
    "\n",
    "act_fun = ActFun.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_update(input, mem, spk, thr, decay, rst):\n",
    "    mem = rst * spk + mem * decay * (1-spk) + input\n",
    "    spike = act_fun(mem - thr)\n",
    "    return mem, spike\n",
    "\n",
    "class RC(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(RC, self).__init__()\n",
    "        input = config.input\n",
    "        hid = config.hid\n",
    "        out = config.output\n",
    "        self.fc_in = nn.Linear(input, hid)\n",
    "        self.A = nn.Parameter(torch.tensor(A_cluster(config)), requires_grad=False) # 邻接矩阵\n",
    "        self.fc_out = nn.Linear(hid, out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        batch, time_step, in_dim = input.shape\n",
    "        hid_mem = torch.empty(batch, config.hid).uniform_(0, 0.1).to('cuda')\n",
    "        hid_spk = sum_spk = torch.zeros(batch, config.hid).to('cuda')\n",
    "        for t in range(time_step):\n",
    "            x = self.fc_in(input[:,t,:])\n",
    "            x = x @ self.A\n",
    "            hid_mem, hid_spk = mem_update(x, hid_mem, hid_spk, config.thr, config.decay, config.rst)\n",
    "            sum_spk += hid_spk\n",
    "        sum_spk /= time_step\n",
    "        out = self.fc_out(sum_spk)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, num_epochs, train_loader, test_loader, device):\n",
    "    train_accs = []\n",
    "    best_acc = 75\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "        for i, (samples, labels) in enumerate(train_loader):\n",
    "            samples = samples.requires_grad_().to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(samples.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tr_acc = test(model, train_loader)\n",
    "        ts_acc = test(model, test_loader)\n",
    "        if ts_acc > best_acc and tr_acc > 75:\n",
    "            best_acc = ts_acc\n",
    "        train_accs.append(tr_acc)\n",
    "        res_str = 'epoch: ' + str(epoch) \\\n",
    "                    + ' Loss: ' + str(loss.item()) \\\n",
    "                    + '. Tr Acc: ' + str(tr_acc)   \\\n",
    "                    + '. Ts Acc: ' + str(ts_acc)   \\\n",
    "                    + '. Time:' + str(time.time()-now)\n",
    "        print(res_str)\n",
    "    return train_accs\n",
    "\n",
    "def test(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        outputs = model(images.to(config.device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.long().cpu()).sum()\n",
    "    accuracy = 100. * correct.numpy() / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss: 1.9954992532730103. Tr Acc: 44.89946051986268. Ts Acc: 41.386925795053. Time:49.33854937553406\n",
      "epoch: 1 Loss: 1.3443876504898071. Tr Acc: 49.26434526728789. Ts Acc: 46.775618374558306. Time:49.394436836242676\n",
      "epoch: 2 Loss: 1.545304298400879. Tr Acc: 55.11280039234919. Ts Acc: 42.40282685512368. Time:47.19324040412903\n",
      "epoch: 3 Loss: 0.8963672518730164. Tr Acc: 57.93281020107896. Ts Acc: 51.63427561837456. Time:48.78626227378845\n",
      "epoch: 4 Loss: 1.4052256345748901. Tr Acc: 66.25796959293771. Ts Acc: 56.22791519434629. Time:50.378888845443726\n",
      "epoch: 5 Loss: 1.1851226091384888. Tr Acc: 67.0426679744973. Ts Acc: 59.80565371024735. Time:46.271888732910156\n",
      "epoch: 6 Loss: 0.8978437185287476. Tr Acc: 63.48700343305542. Ts Acc: 57.72968197879859. Time:49.56532597541809\n",
      "epoch: 7 Loss: 1.0156340599060059. Tr Acc: 63.92839627268269. Ts Acc: 54.15194346289753. Time:47.26534295082092\n",
      "epoch: 8 Loss: 0.8282561302185059. Tr Acc: 69.47032859244727. Ts Acc: 56.97879858657244. Time:53.08135414123535\n",
      "epoch: 9 Loss: 1.1196895837783813. Tr Acc: 70.29180971064247. Ts Acc: 59.31978798586572. Time:48.50258994102478\n",
      "epoch: 10 Loss: 0.9576817750930786. Tr Acc: 70.0465914664051. Ts Acc: 60.5565371024735. Time:54.63874816894531\n",
      "epoch: 11 Loss: 0.9170562028884888. Tr Acc: 68.15841098577734. Ts Acc: 59.67314487632509. Time:57.20793151855469\n",
      "epoch: 12 Loss: 0.7435678839683533. Tr Acc: 75.26974006866111. Ts Acc: 57.15547703180212. Time:44.18034482002258\n",
      "epoch: 13 Loss: 0.5848270654678345. Tr Acc: 73.88425698871995. Ts Acc: 52.38515901060071. Time:39.13250494003296\n",
      "epoch: 14 Loss: 0.8947139978408813. Tr Acc: 72.96468857282981. Ts Acc: 54.90282685512368. Time:38.93271827697754\n",
      "epoch: 15 Loss: 0.5362604856491089. Tr Acc: 74.70573810691515. Ts Acc: 62.94169611307421. Time:39.11629629135132\n",
      "epoch: 16 Loss: 0.8181362152099609. Tr Acc: 76.53261402648357. Ts Acc: 60.5565371024735. Time:40.43246340751648\n",
      "epoch: 17 Loss: 0.6830345392227173. Tr Acc: 74.44825895046591. Ts Acc: 57.90636042402827. Time:39.353984117507935\n",
      "epoch: 18 Loss: 0.5426626801490784. Tr Acc: 76.69200588523786. Ts Acc: 64.09010600706713. Time:41.4632465839386\n",
      "epoch: 19 Loss: 0.582951545715332. Tr Acc: 76.54487493869544. Ts Acc: 57.86219081272085. Time:40.353825092315674\n",
      "epoch: 20 Loss: 0.618518054485321. Tr Acc: 76.86365865620402. Ts Acc: 58.21554770318021. Time:39.13135480880737\n",
      "epoch: 21 Loss: 0.5389150381088257. Tr Acc: 77.17018146150073. Ts Acc: 58.613074204947. Time:39.09812545776367\n",
      "epoch: 22 Loss: 0.7785983681678772. Tr Acc: 78.44531633153507. Ts Acc: 63.91342756183746. Time:39.43647813796997\n",
      "epoch: 23 Loss: 0.7197479009628296. Tr Acc: 76.36096125551741. Ts Acc: 61.042402826855124. Time:38.997151136398315\n",
      "epoch: 24 Loss: 0.42697539925575256. Tr Acc: 78.69053457577243. Ts Acc: 60.5565371024735. Time:39.216334104537964\n",
      "epoch: 25 Loss: 0.4200414717197418. Tr Acc: 78.22461991172143. Ts Acc: 61.57243816254417. Time:38.652843713760376\n",
      "epoch: 26 Loss: 0.5667850375175476. Tr Acc: 78.50662089259441. Ts Acc: 61.57243816254417. Time:39.00233030319214\n",
      "epoch: 27 Loss: 0.5555938482284546. Tr Acc: 82.51593918587542. Ts Acc: 63.82508833922262. Time:39.188312292099\n",
      "epoch: 28 Loss: 0.5488837957382202. Tr Acc: 78.4698381559588. Ts Acc: 62.41166077738516. Time:39.25485944747925\n",
      "epoch: 29 Loss: 0.41258782148361206. Tr Acc: 78.83766552231486. Ts Acc: 62.63250883392226. Time:39.505130767822266\n",
      "epoch: 30 Loss: 0.7121750116348267. Tr Acc: 77.56253065228053. Ts Acc: 60.909893992932865. Time:39.29146766662598\n",
      "epoch: 31 Loss: 0.9801939725875854. Tr Acc: 77.04757233938204. Ts Acc: 59.71731448763251. Time:42.61929202079773\n",
      "epoch: 32 Loss: 0.4661378562450409. Tr Acc: 78.7027954879843. Ts Acc: 64.79681978798587. Time:41.36391091346741\n",
      "epoch: 33 Loss: 0.6492523550987244. Tr Acc: 81.0078469838156. Ts Acc: 60.512367491166074. Time:40.83672499656677\n",
      "epoch: 34 Loss: 0.5088430643081665. Tr Acc: 79.92888670917117. Ts Acc: 64.8851590106007. Time:40.82669997215271\n",
      "epoch: 35 Loss: 0.5747789144515991. Tr Acc: 82.38106915154488. Ts Acc: 62.897526501766784. Time:40.733280181884766\n",
      "epoch: 36 Loss: 0.7824335694313049. Tr Acc: 83.98724865129965. Ts Acc: 65.9452296819788. Time:40.83510875701904\n",
      "epoch: 37 Loss: 0.5465532541275024. Tr Acc: 81.60863168219716. Ts Acc: 65.06183745583039. Time:42.67055058479309\n",
      "epoch: 38 Loss: 0.2826889455318451. Tr Acc: 79.94114762138302. Ts Acc: 60.909893992932865. Time:41.23197674751282\n",
      "epoch: 39 Loss: 0.6389545202255249. Tr Acc: 80.44384502206964. Ts Acc: 62.985865724381625. Time:42.166441917419434\n",
      "epoch: 40 Loss: 0.6913865208625793. Tr Acc: 79.30358018636586. Ts Acc: 61.43992932862191. Time:40.39450430870056\n",
      "epoch: 41 Loss: 0.5251375436782837. Tr Acc: 83.11672388425698. Ts Acc: 61.48409893992933. Time:40.85776376724243\n",
      "epoch: 42 Loss: 0.9685565233230591. Tr Acc: 81.42471799901912. Ts Acc: 65.85689045936395. Time:40.557799100875854\n",
      "epoch: 43 Loss: 0.18022818863391876. Tr Acc: 81.76802354095145. Ts Acc: 63.25088339222615. Time:40.68347883224487\n",
      "epoch: 44 Loss: 0.5303378105163574. Tr Acc: 83.23933300637567. Ts Acc: 62.19081272084806. Time:41.403518199920654\n",
      "epoch: 45 Loss: 0.5932134985923767. Tr Acc: 82.87150564001962. Ts Acc: 61.66077738515901. Time:40.872663736343384\n",
      "epoch: 46 Loss: 0.3622397482395172. Tr Acc: 82.43011280039235. Ts Acc: 61.43992932862191. Time:41.0438551902771\n",
      "epoch: 47 Loss: 0.7923732399940491. Tr Acc: 81.73124080431585. Ts Acc: 66.03356890459364. Time:41.06676721572876\n",
      "epoch: 48 Loss: 0.5483838319778442. Tr Acc: 84.73516429622364. Ts Acc: 65.90106007067138. Time:40.73066234588623\n",
      "epoch: 49 Loss: 0.19568823277950287. Tr Acc: 83.54585581167238. Ts Acc: 68.55123674911661. Time:40.63462543487549\n"
     ]
    }
   ],
   "source": [
    "model = RC().to('cuda')\n",
    "model.fc_in.requires_grad_ = False\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.lr)\n",
    "acc = train(model, optimizer, criterion, config.epoch, train_loader, test_loader, 'cuda')\n",
    "accuracy = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 92.31K\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPS: 1.79M\n",
      "Parameters: 92.31K\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "model = RC().to('cuda')\n",
    "params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(\"Parameters: {:.2f}K\".format(params / 1e3))\n",
    "\n",
    "input = torch.randn(1, 20, 700).cuda() #.to(config.device)\n",
    "flops, params = profile(model.cuda(), inputs=(input,))\n",
    "print(\"FLOPS: {:.2f}M\".format(flops / 1e6))\n",
    "print(\"Parameters: {:.2f}K\".format(params / 1e3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
